# -*- coding: utf-8 -*-
"""23B2158_ME228-Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FiCB0SwzC7ozbWC0Aph0ku8qx9pfVF7I

## Title of the project : Predicting Diabetes Using Machine Learning

## Your name : Santosh Guntuku

## Student ID : 23B2158

## Data Source :  
### National Institute of Diabetes and Digestive and Kidney Diseases  
[Dataset Link](https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset)

## Source of the code : developed completely by me

# Step 1: Import Required Libraries
"""

# Importing libraries to help with data analysis, visualization, and preprocessing
import pandas as pd  # Pandas helps to work with structured data (tables, spreadsheets)
import numpy as np  # NumPy helps with numbers, calculations, and missing data handling
import matplotlib.pyplot as plt  # Matplotlib is used to create graphs and charts
import seaborn as sns  # Seaborn is a visualization library for creating advanced statistical plots
from sklearn.impute import KNNImputer  # KNN Imputer helps to fill in missing values in data
from sklearn.preprocessing import RobustScaler  # Scales data while reducing the effect of extreme values
from scipy.stats import skew  # Measures skewness (how unbalanced the data distribution is)

"""### Why are we doing this?

Data Handling → pandas, numpy help load and modify datasets.

Visualization → matplotlib.pyplot, seaborn help create graphs to understand the data.

Data Cleaning & Preprocessing → KNNImputer fills missing values, RobustScaler normalizes data, and skew checks for data imbalances.

# Step 2: Load the Dataset
"""

# Load the dataset from a CSV file (like an Excel sheet)
data = pd.read_csv("diabetes.csv")

"""### Why are we doing this?

.read_csv("diabetes.csv") reads the dataset and stores it in a variable called data.

Now, we can manipulate the data in Python just like we would in an Excel sheet.

# Step 3: Check Dataset Structure
"""

# Display dataset information
print("Shape of dataset (rows, columns):", data.shape)  # Prints dataset size
print("\nFeature types:")
print(data.dtypes)  # Prints the data type of each column
print("\nFirst 5 rows of dataset:")
print(data.head())  # Shows the first 5 rows of the dataset

"""### Why are we doing this?

.shape → Tells us how many rows (patients) and columns (features) are in the dataset.

.dtypes → Shows whether features are numbers (floats, integers) or text (strings, categories).

.head() → Displays sample data to ensure the dataset is loaded correctly.


"""

# Summary Statistics of Features
print("\nSummary Statistics:")
print(data.describe())

# Additional dataset information
print("\nDataset Info:")
print(data.info())

# Count of unique values per column
print("\nNumber of unique values in each column:")
print(data.nunique())

"""# Step 4: Check for Missing Values"""

# Check if any data is missing in the dataset
print("\nMissing values in dataset:")
print(data.isnull().sum())  # Shows how many missing values exist in each column

"""### Why are we doing this?

Missing data can cause errors in machine learning models.

.isnull().sum() counts missing values in each column.

# Step 5: Identify Wrong Zero Values
"""

# Check how many zero values exist in each column
zero_values = (data == 0).sum()
print("\nZero values in dataset (potential missing data):")
print(zero_values)

"""### Why are we doing this?

Some features should never be zero (like Blood Pressure, BMI).

This step helps us find mistakes in the data that need to be corrected.

# Step 7: Handling Missing Values (Fixing Zero Values)
## Understanding the Problem

In real-world medical data, missing values often appear as zero values, especially for measurements like:

Glucose Level (should never be zero for a living person)

Blood Pressure (a value of zero means the person is dead)

BMI (Body Mass Index) (can’t be zero because every person has some
weight & height)

So, we need to replace these zero values with meaningful estimates instead of just leaving them as zero.

### Identify the Columns That Need Fixing
"""

columns_to_fix = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

"""This tells Python which columns need correction because they shouldn’t have zero values.

### Method 1: Median Imputation (Simple Method)
"""

median_imputed_data = data.copy()  # Create a copy of the dataset (to avoid modifying the original)
for col in columns_to_fix:
    median_imputed_data[col] = median_imputed_data[col].replace(0, median_imputed_data[col].median())

"""### What happens here?

Step 1: We create a copy of the dataset (median_imputed_data) to keep the original data safe.

Step 2: We loop through each column that needs fixing (Glucose, BloodPressure, etc.).

Step 3: We replace zero values with the median of that column.

### Why do we use the median instead of the mean?

Mean (average) is sensitive to outliers (e.g., if one patient has an insulin level of 1000, it will distort the average).

Median is more reliable because it represents the middle value, ignoring extreme outliers.

Example:

Patient	Glucose (Before)	Glucose (After Median Imputation)

1	110	110

2	0	95 (Median)

3	140	140

4	0	95 (Median)

Zero values are now replaced with 95 (the median glucose level), making the data more realistic.

## Method 2: KNN Imputation (Advanced Method)
"""

knn_imputed_data = data.copy()  # Create another copy of the dataset
knn_imputed_data[columns_to_fix] = knn_imputed_data[columns_to_fix].replace(0, np.nan)  # Convert zeroes to NaN
knn_imputer = KNNImputer(n_neighbors=5)  # Use K-Nearest Neighbors to estimate missing values
knn_imputed_data[columns_to_fix] = knn_imputer.fit_transform(knn_imputed_data[columns_to_fix])

"""### What happens here?

Step 1: We make a new copy of the dataset (knn_imputed_data) to keep the original safe.

Step 2: We replace all zero values with NaN (Not a Number) so that the imputer can recognize them as missing.

Step 3: We use KNNImputer(n_neighbors=5), which looks at the 5 closest patients and estimates the missing values based on similar patients.

Step 4: fit_transform() applies the KNN imputer, filling in the missing values.

Example:

Patient	Age	Glucose (Before)	Glucose (After KNN)

1	45	120	120

2	35	0	110 (KNN estimate)

3	50	140	140

4	38	0	115 (KNN estimate)

KNN is more advanced because it doesn’t use a fixed value (like median) but estimates the value based on similar cases.

## Choosing KNN
"""

data = knn_imputed_data

"""# Step 6: Data Visualization (Graphs & Charts)
### Show the Number of Diabetic vs. Non-Diabetic Patients
"""

plt.figure(figsize=(6, 4))
sns.countplot(x='Outcome', data=data, palette='viridis')  # Creates a bar chart
plt.title("Diabetes Outcome Distribution")
plt.xlabel("Outcome (0: Non-Diabetic, 1: Diabetic)")
plt.ylabel("Count")
plt.show()
print("Class Distribution:")
print(data['Outcome'].value_counts())

"""This graph helps understand the balance between diabetic and non-diabetic cases.

If most data is non-diabetic, we may need balancing techniques for fair training.

### For now, there is no immediate need to apply class imbalance techniques like SMOTE, as the dataset contains **268 diabetic cases** and **500 non-diabetic cases**, making it **moderately balanced**. However, if needed, we will consider applying SMOTE or class weighting during further model evaluation and explicitly mention its impact on performance.

## Visualization Plots

### 1. Pairplot (Feature Relationships)
"""

sns.pairplot(data, hue='Outcome', palette='husl')
plt.show()

"""### Pairplot (Scatterplot Matrix)
What It Shows:

Scatterplots between pairs of features, helping to detect patterns and relationships.

If you have a color distinction (e.g., diabetes vs. non-diabetes), you can see class separability.

Key Insights from Data:

Glucose vs. BMI: Likely shows a trend (higher BMI, higher diabetes risk).

Age vs. Outcome: Diabetes may be more common in older individuals.

SkinThickness vs. Insulin: Points may be scattered, indicating weak correlation.

How to Interpret:

If data points are separable, models like Logistic Regression may work well.

If there’s overlap, tree-based models (Random Forest, Decision Tree) may perform better.

### 2. Correlation Heatmap
"""

plt.figure(figsize=(10, 6))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

"""### Correlation Heatmap
What It Shows:

A heatmap represents how strongly features are correlated (values range from -1 to 1).

Positive correlation (closer to +1) → Features increase together.

Negative correlation (closer to -1) → One feature increases while the other decreases.

No correlation (near 0) → Features are independent.

Key Insights from Data:

Glucose vs. Outcome → Likely strongly correlated (higher glucose levels increase diabetes risk).

BMI vs. Outcome → May show moderate correlation.

SkinThickness & Insulin → Often weakly correlated due to missing values or biological variability.

How to Interpret:

Features with high correlation may cause multicollinearity (which affects linear models like Logistic Regression).

If two features are highly correlated, you might drop one (feature selection).

Weakly correlated features may still contribute to complex models like Random Forest or Neural Networks.

### 3. Histograms (Feature Distributions)
"""

data.hist(figsize=(12, 10), bins=20, edgecolor='black')
plt.suptitle("Feature Distributions", fontsize=16)
plt.show()

"""### 4. Boxplots (Outlier Detection)"""

plt.figure(figsize=(12, 8))
for i, column in enumerate(data.columns[:-1], 1):  # Exclude 'Outcome'
    plt.subplot(3, 3, i)
    sns.boxplot(y=data[column], color='teal')
    plt.title(column)
plt.tight_layout()
plt.show()

"""### Feature Distributions (Histograms & Boxplots)
What They Show:

Histograms give an overview of how numerical features (like Glucose, BMI, Insulin, etc.) are distributed.
Boxplots help detect outliers by showing the median, quartiles, and extreme values.

Key Insights from Data:

Glucose & BMI: Likely to be right-skewed since high glucose and BMI values are common in diabetes cases.

Insulin: Often has extreme outliers since insulin levels vary greatly.

Age: Might show a normal or skewed distribution, depending on the dataset.

How to Interpret:

If the histogram shows a normal distribution, StandardScaler is good.
If it’s skewed, RobustScaler (which you're using) is better.
If boxplots show many outliers, consider handling them (e.g., capping extreme values or using log transformation).

# Step 8: Handling Outliers (Extreme Values)
## Understanding the Problem

Some patients might have abnormally high or low values that don’t make sense.

For example:

A BMI of 100 is extremely high and likely an error.

A Blood Pressure of 20 is unrealistically low for a living person.

Such outliers can mislead the model, so we need to remove or correct them.

## Use IQR (Interquartile Range) to Remove Outliers
"""

for col in columns_to_fix:
    q1, q3 = np.percentile(data[col], [25, 75])  # Find 25th and 75th percentiles
    iqr = q3 - q1  # Calculate Interquartile Range
    lower_bound = q1 - 1.5 * iqr  # Define lower limit for outliers
    upper_bound = q3 + 1.5 * iqr  # Define upper limit for outliers
    data[col] = np.clip(data[col], lower_bound, upper_bound)  # Replace extreme values

"""### What happens here?

Step 1: Compute Q1 (25th percentile) and Q3 (75th percentile) for each column.

Step 2: Compute IQR (Interquartile Range = Q3 - Q1).

Step 3: Define acceptable limits (values beyond this range are outliers).

Step 4: Replace outliers with the nearest valid value using np.clip().

Example:

Patient	BMI (Before)	BMI (After IQR Correction)

1	22	22

2	50	40 (Upper Limit)

3	5	18 (Lower Limit)

Outliers are now within a reasonable range!

# Step 9: Fix Skewed Data (Unbalanced Distributions)
## Understanding the Problem

Skewness occurs when the data is not evenly distributed.

Example:

Most people have Glucose levels between 80 and 140, but a few have Glucose levels above 300, making the data skewed.

When the data is skewed, models perform poorly because they expect normal (evenly spread) data.

## Apply Log Transformation to Fix Skewness
"""

skewed_features = data[columns_to_fix].apply(lambda x: skew(x)).sort_values(ascending=False)
for col in skewed_features.index:
    if abs(skewed_features[col]) > 0.5:  # Apply log transformation only if skewness is significant
        data[col] = np.log1p(data[col])  # Log transformation

"""Example of How Log Transformation Works

Glucose Values Before Fixing Skewness

Patient	Glucose (Before)

1	80

2	120

3	140

4	150

5	300 (Skewed)

After Log Transformation

Patient	Glucose (After Log)

1	4.39

2	4.79

3	4.94

4	5.01

5	5.71

Now the values are more balanced and less skewed!

Log transformation smooths out extreme values, making the data more normal.

# Step 10: Scale Data for Machine Learning
### Why Do We Scale Data?

Different features have different ranges, which can confuse machine learning models.

Example:

Blood Pressure ranges from 80 to 140

Glucose ranges from 80 to 300

If we don’t scale them, the model will give more importance to larger numbers!
"""

scaler = RobustScaler()  # Initialize RobustScaler
data[columns_to_fix] = scaler.fit_transform(data[columns_to_fix])  # Apply scaling

"""Ensures all features are on a similar scale, improving model accuracy.

RobustScaler uses median and interquartile range (IQR), making it more robust to outliers.

# Step 11: Final Check
"""

print("\nFinal dataset after preprocessing:")
print(data.head())

"""## Why Are Negative Values Appearing?

Negative values appear because of feature scaling, especially when using RobustScaler.

RobustScaler centers the data around zero by subtracting the median and dividing by the interquartile range (IQR):

𝑋
scaled
=
𝑋
−
Median/
IQR

Example Before Scaling

Patient	Glucose (Original)

1	80

2	100

3	120

4	140

After Robust Scaling

Patient	Glucose (After Scaling)

1	-1.0

2	-0.5

3	0.0

4	1.0

Why is this okay?

The model does not care about the actual values; it only looks at relative differences (patterns).

Even though the values are negative, they still preserve the original relationships.

## FINAL SUMMARY OF DATA
"""

# Summary Statistics of Features
print("\nSummary Statistics:")
print(data.describe())

# Additional dataset information
print("\nDataset Info:")
print(data.info())

# Count of unique values per column
print("\nNumber of unique values in each column:")
print(data.nunique())

"""# Step 12: Additional Required Libraries"""

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve

import joblib
import warnings
warnings.filterwarnings("ignore")

"""## Reasons for Using These Libraries

### from sklearn.model_selection import ...

#### `train_test_split`
- **Purpose**: Splits your dataset into training and testing sets.
- **Why**: To train on one set and test on another to evaluate how well your model generalizes to unseen data.

#### `GridSearchCV`
- **Purpose**: Performs exhaustive search over specified hyperparameter values for an estimator.
- **Why**: Helps you find the best combination of parameters for your model (like tuning knobs).

#### `cross_val_score`
- **Purpose**: Evaluates a model using cross-validation (e.g., 5-fold CV).
- **Why**: More reliable estimate of model performance by training/testing on multiple different splits.


---

### Models

#### `LogisticRegression`
- **Purpose**: A linear model for binary classification.
- **Why**: Simple, interpretable, and works well for linearly separable data.

#### `DecisionTreeClassifier`
- **Purpose**: A tree-based classifier.
- **Why**: Captures non-linear relationships and easy to visualize.

#### `RandomForestClassifier`
- **Purpose**: An ensemble of decision trees (bagging).
- **Why**: More accurate and stable than a single decision tree.

#### `MLPClassifier`
- **Purpose**: Multi-layer perceptron (a type of neural network).
- **Why**: Can learn complex, non-linear decision boundaries.

#### `SVC` (Support Vector Classifier)
- **Purpose**: Support Vector Machine for classification.
- **Why**: Good for high-dimensional spaces and robust to overfitting.


---

### Evaluation Metrics

#### `accuracy_score`

- **What it is**:  
  The ratio of correctly predicted instances to the total instances.
  
- **Why it's needed**:  
  It’s the most intuitive metric — it tells you how often your model is correct.

- **Limitation**:  
  In imbalanced datasets, accuracy can be misleading.  
  For example, if 90% of the patients don’t have diabetes, a model that always predicts “no diabetes” will still have 90% accuracy — but it's useless.


#### `precision_score`

- **What it is**:  
  Out of the predicted positive cases, how many are actually positive?  

  [{Precision} = {True Positives} \ {{True Positives} + {False Positives}}]

- **Why it's needed**:  
  When false positives are costly — like falsely diagnosing diabetes — precision matters.  
  You want to be confident that when the model says “diabetic,” it’s likely correct.


#### `recall_score` (Sensitivity or True Positive Rate)

- **What it is**:  
  Out of all actual positive cases, how many did the model correctly identify?

  [{Recall} = {True Positives} / {{True Positives} + {False Negatives}}]

- **Why it's needed**:  
  When false negatives are dangerous — like missing a diabetic patient — recall is key.  
  High recall means the model catches most diabetic cases.


#### `f1_score`

- **What it is**:  
  The harmonic mean of precision and recall.

  [F_1 = 2*{Precision}*{Recall} \ {{Precision} + {Recall}}]

- **Why it's needed**:  
  A balanced measure when you need a trade-off between precision and recall.  
  Useful in medical cases where both false positives and false negatives matter.


#### `roc_auc_score`

- **What it is**:  
  The area under the Receiver Operating Characteristic (ROC) curve.

- **Why it's needed**:  
  AUC shows how well the model separates the classes.  
  AUC = 1 means perfect, AUC = 0.5 means no better than random guessing.  
  It's especially good for comparing models, regardless of threshold.


#### `confusion_matrix`

- **What it is**:  
  A table showing:
  - True Positives (TP)
  - False Positives (FP)
  - True Negatives (TN)
  - False Negatives (FN)

- **Why it's needed**:  
  It gives a complete picture of model performance — not just a score, but the types of mistakes being made.


#### `classification_report`

- **What it is**:  
  A summary that includes:
  - Precision
  - Recall
  - F1-score
  - Support (number of true instances for each class)

- **Why it's needed**:  
  Quick, readable way to compare all metrics for each class (especially in multi-class problems).


---

### Other Utilities

#### `import joblib`

- **Purpose**: To save (`.pkl` file) and load ML models.
- **Why**: So you can reuse trained models without retraining every time.


#### `import warnings`  
`warnings.filterwarnings("ignore")`

- **Purpose**: Suppresses warning messages.
- **Why**: To keep the output clean, especially in notebooks or presentations (though be cautious — warnings can be helpful).

# Step 13: Train-Test split
"""

# Features and target split
X = data.drop("Outcome", axis=1)
y = data["Outcome"]

# Train-Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""### Reason:

We want to see how well the model performs on new, unseen data.

Training data is used to teach the model.

Test data is used to check if the model generalizes well.

# Step 14: Models

### Information Regarding The Models Used Below

---

#### 1. **Logistic Regression**

Logistic Regression is one of the simplest and most widely used machine learning algorithms for classification tasks. Even though it has “regression” in its name, it’s actually used for predicting categorical outcomes, such as whether a person has diabetes (yes/no). It works by calculating the probability that a given input belongs to a certain class using a mathematical function called the **sigmoid or logistic function**, which squashes the output between 0 and 1. Internally, it assigns weights to each input feature (like BMI, Glucose level, etc.) and tries to find the best weights such that the model can correctly classify most of the data. It assumes a linear relationship between the input features and the log-odds of the outcome. It is **interpretable**, **fast to train**, and works well when the data is **linearly separable**, but it struggles when the relationship between features and target is non-linear or more complex.

---

#### 2. **Random Forest**

Random Forest is an **ensemble learning** method, which means it combines multiple individual models to make a better, more accurate overall model. In this case, it builds many **decision trees** (hence the term "forest") during training and outputs the **majority vote** (for classification) or average prediction (for regression). Each decision tree is trained on a **random subset of data and features**, making them slightly different. This randomness helps to **reduce overfitting** and improves generalization. Random Forest handles both **linear and non-linear** data well, works with both categorical and continuous data, and provides **high accuracy**. It also gives us an idea of **which features are most important**. However, it can be **slow and memory-intensive** with very large datasets and lacks interpretability compared to simpler models.

---

#### 3. **K-Nearest Neighbors (KNN)**

KNN is one of the most **intuitive algorithms**. It doesn’t actually learn from the training data during a training phase like others do. Instead, it stores the entire dataset and makes predictions **only when needed** (this is called a **lazy learner**). When a new input comes in, KNN looks at the ‘k’ closest data points (neighbors) in the training data and assigns the majority class among them. Distance is usually measured using **Euclidean distance**. It works well for **small datasets** with clear boundaries but becomes **slow and inefficient** for large datasets, and it’s **sensitive to the scale** of data. It also struggles with **high-dimensional data** and **imbalanced datasets** but can give solid results when tuned correctly.

---

#### 4. **Decision Tree**

A Decision Tree is a **flowchart-like structure** where internal nodes represent decisions based on a feature (like “Is Glucose > 125?”), branches represent outcomes of these decisions, and leaf nodes represent final predictions. It splits the dataset **recursively** by choosing the feature that best separates the classes using measures like **Gini Index** or **Entropy**. It’s **easy to visualize, interpret, and explain**, making it ideal for understanding the logic behind predictions. But decision trees are **prone to overfitting**—memorizing the training data and performing poorly on new, unseen data. This can be mitigated with techniques like **pruning** or by using **tree ensembles** (like Random Forest or Gradient Boosting).

---

#### 5. **AdaBoost (Adaptive Boosting)**

AdaBoost is another **ensemble technique**, but it works differently. Instead of training multiple models independently, it trains them **sequentially**. Each new model tries to **fix the mistakes** made by the previous one by focusing more on the **misclassified samples**. It combines the outputs of all models using a **weighted vote**. The “boosting” refers to its ability to **convert weak learners** (simple models that perform slightly better than random guessing, like small decision trees) into a strong learner. AdaBoost often provides **better accuracy** than a single tree, especially on structured data. However, it's **sensitive to noise and outliers** and may overfit if not tuned properly.

---

#### 6. **Gradient Boosting**

Gradient Boosting is a more **sophisticated and powerful version of boosting**. Like AdaBoost, it builds models **sequentially**, but it uses a method based on **gradient descent** to minimize the errors made by the model. Each model in the sequence is trained to **predict the residuals (errors)** of the previous model. It’s highly customizable and can handle a wide variety of data types and problems. It often achieves **top performance** in competitions and real-world applications. However, it requires **more time to train**, is **sensitive to hyperparameters**, and can **overfit** if not controlled. Modern implementations like **XGBoost** and **LightGBM** are built on this idea.

---

#### 7. **XGBoost (Extreme Gradient Boosting)**

XGBoost is a very **fast and efficient implementation** of Gradient Boosting. It’s optimized for **speed and performance** through features like **parallel computation**, **regularization** (to avoid overfitting), **missing value handling**, and **tree pruning**. It often wins machine learning competitions due to its **scalability and accuracy**. XGBoost uses a more **regularized model** formalization to control overfitting and handles **large-scale data** effectively. It is suitable for both **classification and regression** tasks. Although it's a bit **complex to understand** at first, it’s one of the **most powerful models** in modern data science workflows.

---

#### 8. **LightGBM (Light Gradient Boosting Machine)**

LightGBM is a newer **gradient boosting framework** developed by Microsoft that is **faster and more efficient** than XGBoost, especially on large datasets. It uses **histogram-based learning** and **leaf-wise tree growth**, which makes it faster and reduces memory usage. It also handles **categorical features natively** without needing one-hot encoding. LightGBM can sometimes **overfit small datasets** due to aggressive leaf-wise growth but performs **exceptionally well on high-dimensional, large-scale data**. It's also capable of handling **imbalance in data** well through **custom loss functions and weight adjustments**.

---

#### 9. **Support Vector Machine (SVM)**

SVM is a powerful model used for classification that tries to find the **optimal boundary (hyperplane)** that best separates the classes. It works by finding the hyperplane that has the **maximum margin** (i.e., the largest distance between the boundary and the closest data points from each class, known as **support vectors**). For data that is not linearly separable, it uses something called the **kernel trick** to project data into higher dimensions where a linear separator can be found. SVMs are very effective in **high-dimensional spaces** and work well for **clear margin separation**. However, they are **computationally intensive**, especially for large datasets, and they don’t perform well when the data has **overlapping classes**.

---

#### 10. **MLPClassifier (Multi-Layer Perceptron / Neural Network)**

MLPClassifier is a type of **Artificial Neural Network (ANN)**, specifically a **feedforward network** that consists of multiple layers: **input**, **hidden**, and **output layers**. Each layer contains **neurons** that are connected to neurons in the next layer. During training, the model learns **weights** for each connection using a process called **backpropagation**, where it adjusts weights based on the **error made in the previous prediction**. The model tries to minimize this error using an **optimization algorithm** like **Stochastic Gradient Descent**. MLP can learn **complex, non-linear relationships** in the data and is very **flexible**. However, it requires **more data and time** to train, and it’s often considered a **black box** due to its lack of interpretability. It's widely used in fields like **computer vision** and **natural language processing**.

---

### Machine Learning Models Comparison Table

Below is a summary of each model used in this project, including its advantages, disadvantages, and best-suited scenarios. This helps understand when and why each model is effective.

---

### 1. **Logistic Regression**
- **Advantages**:
  - Simple and easy to interpret
  - Fast to train and predict
  - Outputs class probabilities
  - Works well with linearly separable data
- **Disadvantages**:
  - Assumes a linear relationship between features and output
  - Can underperform with complex data patterns
  - Sensitive to outliers and irrelevant features
- **Best For**: Baseline models, medical data where interpretability is important

---

### 2. **Decision Tree**
- **Advantages**:
  - Easy to understand and visualize
  - Can handle both numerical and categorical data
  - Non-linear relationships handled well
- **Disadvantages**:
  - Prone to overfitting
  - Unstable with small data variations
- **Best For**: Small to medium datasets, clear logic flow in decision making

---

### 3. **Random Forest**
- **Advantages**:
  - Combines many decision trees (ensemble method)
  - Reduces overfitting and improves generalization
  - Handles missing values and outliers better
- **Disadvantages**:
  - Less interpretable than a single decision tree
  - Requires more memory and computational power
- **Best For**: Tabular data, when accuracy is more important than interpretability

---

### 4. **K-Nearest Neighbors (KNN)**
- **Advantages**:
  - Very simple and easy to implement
  - No training time
  - Works well with small datasets
- **Disadvantages**:
  - Slow during prediction (needs to compare with all training data)
  - Sensitive to irrelevant features and feature scaling
- **Best For**: Small datasets, or when you want a quick, non-parametric method

---

### 5. **AdaBoost (Adaptive Boosting)**
- **Advantages**:
  - Turns weak learners into strong ones
  - Works well with a wide variety of data
  - Often improves generalization performance
- **Disadvantages**:
  - Sensitive to noisy data and outliers
  - Slower than individual models
- **Best For**: Binary classification problems where boosting helps a weak learner

---

### 6. **Gradient Boosting**
- **Advantages**:
  - Extremely powerful and accurate
  - Captures complex, non-linear relationships
  - Works well with many different types of features
- **Disadvantages**:
  - Requires careful tuning
  - Computationally intensive and slow to train
- **Best For**: Structured/tabular data, competitions, high-performance systems

---

### 7. **XGBoost**
- **Advantages**:
  - Very fast and efficient
  - Built-in regularization to reduce overfitting
  - Can handle missing values automatically
- **Disadvantages**:
  - Harder to interpret
  - Needs tuning of many hyperparameters
- **Best For**: Large datasets, Kaggle competitions, production ML pipelines

---

### 8. **LightGBM**
- **Advantages**:
  - Faster than XGBoost on large datasets
  - Low memory usage
  - Handles categorical features natively
- **Disadvantages**:
  - Can overfit if not tuned properly
  - Not ideal for small datasets
- **Best For**: Big data, high-dimensional features, time-critical applications

---

### 9. **Support Vector Machine (SVM)**
- **Advantages**:
  - Great for high-dimensional spaces
  - Can handle non-linear data using kernel trick
  - Very robust when well-tuned
- **Disadvantages**:
  - Slow with large datasets
  - Needs careful feature scaling
- **Best For**: Text classification, bioinformatics, small-to-medium datasets

---

### 10. **MLP (Multilayer Perceptron / Neural Network)**
- **Advantages**:
  - Learns complex patterns and interactions
  - Flexible architecture
  - Capable of approximating any function with enough neurons
- **Disadvantages**:
  - Requires a lot of data to train effectively
  - Hard to interpret (black-box)
  - Sensitive to hyperparameters and slow to train
- **Best For**: Problems with complex relationships, deep learning tasks

---

## 1. Logistic Regression
"""

logistic = LogisticRegression()
logistic.fit(X_train, y_train)

# Predictions
logistic_pred = logistic.predict(X_test)
logistic_proba = logistic.predict_proba(X_test)[:, 1]

# Evaluation
print("Logistic Regression:\n")
print(confusion_matrix(y_test, logistic_pred))
print(classification_report(y_test, logistic_pred))
print("Accuracy:", accuracy_score(y_test, logistic_pred))
print("ROC AUC:", roc_auc_score(y_test, logistic_proba))

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, logistic_proba)
plt.plot(fpr, tpr, label="Logistic Regression (AUC = {:.2f})".format(roc_auc_score(y_test, logistic_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - Logistic Regression")
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.legend()
plt.grid(True)
plt.show()

# Save model
joblib.dump(logistic, "model_logistic.joblib")

"""###  Logistic Regression: Training, Evaluation & ROC Curve

In this section, we train a **Logistic Regression** model, make predictions, evaluate its performance using key metrics, plot its ROC Curve, and finally save the model.

---

#### 1. Train the Logistic Regression Model

```python
logistic = LogisticRegression()
logistic.fit(X_train, y_train)
```

- We initialize a basic `LogisticRegression` model with default settings.
- Then, we train the model on the training data (`X_train`, `y_train`).

---

#### 2. Make Predictions

```python
logistic_pred = logistic.predict(X_test)
logistic_proba = logistic.predict_proba(X_test)[:, 1]
```

- `.predict()` gives the predicted class labels (0 or 1) for the test set.
- `.predict_proba()` returns the probability estimates; we use `[:, 1]` to extract the probability of the positive class (i.e., class 1 = "diabetic").

---

#### 3. Evaluate the Model

```python
print(confusion_matrix(y_test, logistic_pred))
print(classification_report(y_test, logistic_pred))
print("Accuracy:", accuracy_score(y_test, logistic_pred))
print("ROC AUC:", roc_auc_score(y_test, logistic_proba))
```

- **Confusion matrix** shows true positives, false positives, true negatives, and false negatives.
- **Classification report** gives metrics like precision, recall, f1-score, and support for each class.
- **Accuracy** is the ratio of correct predictions to total predictions.
- **ROC AUC** tells how well the model separates the two classes; higher is better (1.0 = perfect).

---

#### 4. Plot the ROC Curve

```python
fpr, tpr, _ = roc_curve(y_test, logistic_proba)
```

- Calculates **False Positive Rate (FPR)** and **True Positive Rate (TPR)** at various thresholds for the ROC curve.

```python
plt.plot(fpr, tpr, label="Logistic Regression (AUC = {:.2f})".format(roc_auc_score(y_test, logistic_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - Logistic Regression")
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.legend()
plt.grid(True)
plt.show()
```

- We draw the **ROC curve** to visualize how well the model balances sensitivity and specificity.
- The diagonal line represents random guessing (AUC = 0.5); our model should ideally be above this line.

---

#### 5. Save the Trained Model

```python
joblib.dump(logistic, "model_logistic.joblib")
```

- Saves the trained logistic regression model to disk as a `.joblib` file so it can be reused later without retraining.

---

This simple yet powerful baseline model is useful as a reference when comparing the performance of more complex machine learning models like Random Forest, XGBoost, and Neural Networks.

## 2. Random Forest
"""

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)
rf_proba = rf.predict_proba(X_test)[:, 1]

print("\nRandom Forest:\n")
print(confusion_matrix(y_test, rf_pred))
print(classification_report(y_test, rf_pred))
print("Accuracy:", accuracy_score(y_test, rf_pred))
print("ROC AUC:", roc_auc_score(y_test, rf_proba))

fpr, tpr, _ = roc_curve(y_test, rf_proba)
plt.plot(fpr, tpr, label="Random Forest (AUC = {:.2f})".format(roc_auc_score(y_test, rf_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - Random Forest")
plt.legend()
plt.grid()
plt.show()

joblib.dump(rf, "model_rf.joblib")

"""###  Random Forest: Training, Evaluation & ROC Curve

In this section, we train a **Random Forest Classifier**, evaluate its performance, visualize the ROC Curve, and save the trained model for future use.

---

#### 1. Train the Random Forest Model

```python
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
```

- We create a `RandomForestClassifier` object using default parameters.
- Then we fit the model on the training data (`X_train`, `y_train`), allowing it to learn patterns for predicting diabetes.

---

#### 2. Make Predictions on Test Data

```python
rf_pred = rf.predict(X_test)
rf_proba = rf.predict_proba(X_test)[:, 1]
```

- `rf_pred`: Predicts the final class labels (0 or 1) for each test instance.
- `rf_proba`: Returns the probability of class 1 (positive case — likely diabetic), used for calculating ROC AUC.

---

#### 3. Evaluate Model Performance

```python
print(confusion_matrix(y_test, rf_pred))
print(classification_report(y_test, rf_pred))
print("Accuracy:", accuracy_score(y_test, rf_pred))
print("ROC AUC:", roc_auc_score(y_test, rf_proba))
```

- **Confusion Matrix**: Gives insight into correct vs incorrect classifications.
- **Classification Report**: Displays precision, recall, F1-score, and support for each class.
- **Accuracy**: Percentage of all correct predictions.
- **ROC AUC**: Measures the model’s ability to distinguish between the classes (0.5 = random, 1.0 = perfect).

---

#### 4. Plot ROC Curve

```python
fpr, tpr, _ = roc_curve(y_test, rf_proba)
```

- Computes **False Positive Rate (FPR)** and **True Positive Rate (TPR)** for various thresholds to create the ROC curve.

```python
plt.plot(fpr, tpr, label="Random Forest (AUC = {:.2f})".format(roc_auc_score(y_test, rf_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - Random Forest")
plt.legend()
plt.grid()
plt.show()
```

- The **ROC Curve** shows the trade-off between sensitivity and specificity.
- The dashed diagonal line represents a random guess. A good model curve lies well above it.

---

#### 5. Save the Trained Model

```python
joblib.dump(rf, "model_rf.joblib")
```

- Saves the trained Random Forest model in `.joblib` format, making it easy to load later for predictions without retraining.

---

 Random Forest is a powerful ensemble model that builds multiple decision trees and averages their results, reducing overfitting and improving accuracy.

## 3. K-Nearest Neighbors (KNN)
"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)
knn_proba = knn.predict_proba(X_test)[:, 1]

print("\nK-Nearest Neighbors:\n")
print(confusion_matrix(y_test, knn_pred))
print(classification_report(y_test, knn_pred))
print("Accuracy:", accuracy_score(y_test, knn_pred))
print("ROC AUC:", roc_auc_score(y_test, knn_proba))

fpr, tpr, _ = roc_curve(y_test, knn_proba)
plt.plot(fpr, tpr, label="KNN (AUC = {:.2f})".format(roc_auc_score(y_test, knn_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - KNN")
plt.legend()
plt.grid()
plt.show()

joblib.dump(knn, "model_knn.joblib")

"""###  K-Nearest Neighbors (KNN): Training, Evaluation & ROC Curve

In this section, we apply the **K-Nearest Neighbors classifier** to our diabetes dataset, evaluate its performance, visualize the ROC curve, and save the trained model.

---

#### 1. Train the KNN Classifier

```python
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
```

- We create a KNN model using default settings (number of neighbors = 5).
- It **memorizes** the training data (KNN is a lazy learner) and does not learn a function explicitly.

---

#### 2. Predict Test Labels and Probabilities

```python
knn_pred = knn.predict(X_test)
knn_proba = knn.predict_proba(X_test)[:, 1]
```

- `knn_pred`: Predicts class labels (0 or 1) based on the **majority vote** of k-nearest training points.
- `knn_proba`: Returns probabilities for class 1 (diabetic), which we need to calculate the ROC AUC.

---

#### 3. Evaluate Model Performance

```python
print(confusion_matrix(y_test, knn_pred))
print(classification_report(y_test, knn_pred))
print("Accuracy:", accuracy_score(y_test, knn_pred))
print("ROC AUC:", roc_auc_score(y_test, knn_proba))
```

- **Confusion Matrix**: Shows how many correct and incorrect predictions were made.
- **Classification Report**: Provides metrics like precision, recall, F1-score.
- **Accuracy**: Overall correct predictions as a percentage.
- **ROC AUC**: A higher AUC means better separation of diabetic vs non-diabetic classes.

---

#### 4. Plot ROC Curve

```python
fpr, tpr, _ = roc_curve(y_test, knn_proba)
```

- Calculates values needed to plot the **Receiver Operating Characteristic (ROC)** curve.

```python
plt.plot(fpr, tpr, label="KNN (AUC = {:.2f})".format(roc_auc_score(y_test, knn_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - KNN")
plt.legend()
plt.grid()
plt.show()
```

- The ROC Curve visualizes the trade-off between True Positive Rate and False Positive Rate at different thresholds.
- AUC closer to 1 indicates strong performance.

---

#### 5. Save the Trained Model

```python
joblib.dump(knn, "model_knn.joblib")
```

- Saves the trained KNN model so you can load it later and use it for predictions without retraining.

---

 **KNN** is simple and effective but can be slow with large datasets since it stores all training data and compares new points during prediction.

## 4. Decision Tree
"""

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
dt_pred = dt.predict(X_test)
dt_proba = dt.predict_proba(X_test)[:, 1]

print("\nDecision Tree:\n")
print(confusion_matrix(y_test, dt_pred))
print(classification_report(y_test, dt_pred))
print("Accuracy:", accuracy_score(y_test, dt_pred))
print("ROC AUC:", roc_auc_score(y_test, dt_proba))

fpr, tpr, _ = roc_curve(y_test, dt_proba)
plt.plot(fpr, tpr, label="Decision Tree (AUC = {:.2f})".format(roc_auc_score(y_test, dt_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - Decision Tree")
plt.legend()
plt.grid()
plt.show()

joblib.dump(dt, "model_dt.joblib")

"""###  Decision Tree: Training, Evaluation & ROC Curve

This section implements a **Decision Tree Classifier** to model the diabetes dataset, evaluates its performance, plots the ROC curve, and saves the trained model.

---

#### 1. Train the Decision Tree Classifier

```python
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
```

- A Decision Tree splits the data based on feature thresholds to form a tree-like structure.
- Each decision node chooses the best split to reduce impurity (using Gini by default).

---

#### 2. Make Predictions

```python
dt_pred = dt.predict(X_test)
dt_proba = dt.predict_proba(X_test)[:, 1]
```

- `dt_pred`: Predicted labels (0 or 1) for the test data.
- `dt_proba`: Probability of class 1 (diabetic), used for ROC curve and AUC.

---

#### 3. Evaluate Model Performance

```python
print(confusion_matrix(y_test, dt_pred))
print(classification_report(y_test, dt_pred))
print("Accuracy:", accuracy_score(y_test, dt_pred))
print("ROC AUC:", roc_auc_score(y_test, dt_proba))
```

- **Confusion Matrix**: True vs predicted outcomes.
- **Classification Report**: Shows precision, recall, F1-score.
- **Accuracy**: Proportion of total correct predictions.
- **ROC AUC**: Measures how well the model distinguishes between classes.

---

#### 4. Plot ROC Curve

```python
fpr, tpr, _ = roc_curve(y_test, dt_proba)
plt.plot(fpr, tpr, label="Decision Tree (AUC = {:.2f})".format(roc_auc_score(y_test, dt_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - Decision Tree")
plt.legend()
plt.grid()
plt.show()
```

- ROC Curve plots True Positive Rate vs False Positive Rate.
- The closer the curve to the top-left corner, the better the model.

---

#### 5. Save the Trained Model

```python
joblib.dump(dt, "model_dt.joblib")
```

- Saves the decision tree model to disk for reuse without retraining.

---

 **Decision Trees** are highly interpretable and fast, but may **overfit** without pruning or tuning.

## 5. AdaBoost
"""

from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier()
ada.fit(X_train, y_train)
ada_pred = ada.predict(X_test)
ada_proba = ada.predict_proba(X_test)[:, 1]

print("\nAdaBoost:\n")
print(confusion_matrix(y_test, ada_pred))
print(classification_report(y_test, ada_pred))
print("Accuracy:", accuracy_score(y_test, ada_pred))
print("ROC AUC:", roc_auc_score(y_test, ada_proba))

fpr, tpr, _ = roc_curve(y_test, ada_proba)
plt.plot(fpr, tpr, label="AdaBoost (AUC = {:.2f})".format(roc_auc_score(y_test, ada_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - AdaBoost")
plt.legend()
plt.grid()
plt.show()

joblib.dump(ada, "model_adaboost.joblib")

"""### AdaBoost Classifier: Training, Evaluation & ROC Curve

This section demonstrates how to use **AdaBoost (Adaptive Boosting)** for classifying diabetes data, followed by evaluation, visualization, and model saving.

---

#### 1. Train the AdaBoost Classifier

```python
ada = AdaBoostClassifier()
ada.fit(X_train, y_train)
```

- **AdaBoost** combines multiple weak learners (usually shallow decision trees) into a strong classifier.
- It focuses more on the samples misclassified by previous models by adjusting their weights.

---

#### 2. Make Predictions

```python
ada_pred = ada.predict(X_test)
ada_proba = ada.predict_proba(X_test)[:, 1]
```

- `ada_pred`: Predicted class labels for test data.
- `ada_proba`: Predicted probabilities for class 1 (used for AUC and ROC curve).

---

#### 3. Evaluate Model Performance

```python
print(confusion_matrix(y_test, ada_pred))
print(classification_report(y_test, ada_pred))
print("Accuracy:", accuracy_score(y_test, ada_pred))
print("ROC AUC:", roc_auc_score(y_test, ada_proba))
```

- **Confusion Matrix**: Breakdown of correct and incorrect predictions.
- **Classification Report**: Includes precision, recall, F1-score.
- **Accuracy**: Overall percentage of correct predictions.
- **ROC AUC**: Measures discrimination capability of the model.

---

#### 4. Plot ROC Curve

```python
fpr, tpr, _ = roc_curve(y_test, ada_proba)
plt.plot(fpr, tpr, label="AdaBoost (AUC = {:.2f})".format(roc_auc_score(y_test, ada_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - AdaBoost")
plt.legend()
plt.grid()
plt.show()
```

- ROC Curve visualizes the model's performance across all thresholds.
- AUC closer to 1.0 indicates a strong classifier.

---

#### 5. Save the Trained Model

```python
joblib.dump(ada, "model_adaboost.joblib")
```

- This saves the trained AdaBoost model to a file, allowing reuse without retraining.

---

 **AdaBoost** is simple yet powerful, especially when base models are weak learners.
It can be sensitive to noisy data and outliers.

## 6. Gradient Boosting
"""

from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier()
gb.fit(X_train, y_train)
gb_pred = gb.predict(X_test)
gb_proba = gb.predict_proba(X_test)[:, 1]

print("\nGradient Boosting:\n")
print(confusion_matrix(y_test, gb_pred))
print(classification_report(y_test, gb_pred))
print("Accuracy:", accuracy_score(y_test, gb_pred))
print("ROC AUC:", roc_auc_score(y_test, gb_proba))

fpr, tpr, _ = roc_curve(y_test, gb_proba)
plt.plot(fpr, tpr, label="Gradient Boosting (AUC = {:.2f})".format(roc_auc_score(y_test, gb_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - Gradient Boosting")
plt.legend()
plt.grid()
plt.show()

joblib.dump(gb, "model_gb.joblib")

"""###  Gradient Boosting Classifier: Training, Evaluation & Visualization

This section uses **Gradient Boosting**, a powerful ensemble technique, to build and evaluate a predictive model for diabetes detection.

---

#### 1. Train the Gradient Boosting Model

```python
gb = GradientBoostingClassifier()
gb.fit(X_train, y_train)
```

- **Gradient Boosting** builds models sequentially, where each new model tries to correct the errors made by the previous one.
- It uses **decision trees** as weak learners and **gradient descent** to minimize the loss function.

---

#### 2. Generate Predictions

```python
gb_pred = gb.predict(X_test)
gb_proba = gb.predict_proba(X_test)[:, 1]
```

- `gb_pred`: Predicted class labels (0 or 1).
- `gb_proba`: Probability estimates for the positive class (used for ROC curve and AUC).

---

#### 3. Evaluate the Model

```python
print(confusion_matrix(y_test, gb_pred))
print(classification_report(y_test, gb_pred))
print("Accuracy:", accuracy_score(y_test, gb_pred))
print("ROC AUC:", roc_auc_score(y_test, gb_proba))
```

- **Confusion Matrix**: Shows TP, TN, FP, FN counts.
- **Classification Report**: Displays precision, recall, F1-score.
- **Accuracy**: Fraction of correct predictions.
- **ROC AUC**: Area under the ROC curve, measuring classification quality.

---

#### 4. Plot ROC Curve

```python
fpr, tpr, _ = roc_curve(y_test, gb_proba)
plt.plot(fpr, tpr, label="Gradient Boosting (AUC = {:.2f})".format(roc_auc_score(y_test, gb_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - Gradient Boosting")
plt.legend()
plt.grid()
plt.show()
```

- The ROC curve shows the trade-off between true positive rate and false positive rate.
- AUC closer to 1.0 indicates excellent model performance.

---

#### 5. Save the Model

```python
joblib.dump(gb, "model_gb.joblib")
```

- Saves the trained model for future use without retraining.

---

 **Gradient Boosting** is highly effective on structured/tabular data and often outperforms simpler models.
 It can overfit if not properly tuned and may require longer training time.

## 7. XGBoost
"""

from xgboost import XGBClassifier

xgb = XGBClassifier(eval_metric='logloss')
xgb.fit(X_train, y_train)
xgb_pred = xgb.predict(X_test)
xgb_proba = xgb.predict_proba(X_test)[:, 1]

print("\nXGBoost:\n")
print(confusion_matrix(y_test, xgb_pred))
print(classification_report(y_test, xgb_pred))
print("Accuracy:", accuracy_score(y_test, xgb_pred))
print("ROC AUC:", roc_auc_score(y_test, xgb_proba))

fpr, tpr, _ = roc_curve(y_test, xgb_proba)
plt.plot(fpr, tpr, label="XGBoost (AUC = {:.2f})".format(roc_auc_score(y_test, xgb_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - XGBoost")
plt.legend()
plt.grid()
plt.show()

joblib.dump(xgb, "model_xgb.joblib")

"""### XGBoost Classifier: Training, Evaluation & ROC Analysis

This section utilizes **XGBoost (Extreme Gradient Boosting)**, one of the most powerful and efficient gradient boosting frameworks, to predict diabetes.

---

#### 1. Initialize and Train the Model

```python
xgb = XGBClassifier(eval_metric='logloss')
xgb.fit(X_train, y_train)
```

- **XGBoostClassifier** is an optimized, scalable implementation of gradient boosting.
- `eval_metric='logloss'` ensures log loss is used for evaluation during training.

---

#### 2. Make Predictions

```python
xgb_pred = xgb.predict(X_test)
xgb_proba = xgb.predict_proba(X_test)[:, 1]
```

- `xgb_pred`: Predicted class labels.
- `xgb_proba`: Predicted probability for the positive class (needed for ROC AUC).

---

#### 3. Evaluate Performance

```python
print(confusion_matrix(y_test, xgb_pred))
print(classification_report(y_test, xgb_pred))
print("Accuracy:", accuracy_score(y_test, xgb_pred))
print("ROC AUC:", roc_auc_score(y_test, xgb_proba))
```

- **Confusion Matrix**: Shows model's errors and correct predictions.
- **Classification Report**: Includes precision, recall, and F1-score.
- **Accuracy**: Overall prediction correctness.
- **ROC AUC**: Measures ability to distinguish between classes — higher is better.

---

#### 4. Plot ROC Curve

```python
fpr, tpr, _ = roc_curve(y_test, xgb_proba)
plt.plot(fpr, tpr, label="XGBoost (AUC = {:.2f})".format(roc_auc_score(y_test, xgb_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - XGBoost")
plt.legend()
plt.grid()
plt.show()
```

- Visualizes the classifier’s performance across thresholds.
- The closer the ROC curve hugs the top-left, the better the model.

---

#### 5. Save the Trained Model

```python
joblib.dump(xgb, "model_xgb.joblib")
```

- Stores the trained model to disk for later reuse without retraining.

---

 **XGBoost** is robust to overfitting, supports regularization, and is ideal for structured/tabular data tasks.

## 8. LightGBM
"""

from lightgbm import LGBMClassifier

lgbm = LGBMClassifier()
lgbm.fit(X_train, y_train)
lgbm_pred = lgbm.predict(X_test)
lgbm_proba = lgbm.predict_proba(X_test)[:, 1]

print("\nLightGBM:\n")
print(confusion_matrix(y_test, lgbm_pred))
print(classification_report(y_test, lgbm_pred))
print("Accuracy:", accuracy_score(y_test, lgbm_pred))
print("ROC AUC:", roc_auc_score(y_test, lgbm_proba))

fpr, tpr, _ = roc_curve(y_test, lgbm_proba)
plt.plot(fpr, tpr, label="LightGBM (AUC = {:.2f})".format(roc_auc_score(y_test, lgbm_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - LightGBM")
plt.legend()
plt.grid()
plt.show()

joblib.dump(lgbm, "model_lgbm.joblib")

"""###  LightGBM Classifier: Training, Evaluation & ROC Analysis

This section uses **LightGBM (Light Gradient Boosting Machine)** — a fast, distributed, high-performance gradient boosting framework optimized for speed and efficiency.

---

#### 1. Initialize and Train the Model

```python
lgbm = LGBMClassifier()
lgbm.fit(X_train, y_train)
```

- `LGBMClassifier()` builds an efficient tree-based model using histogram-based learning.
- Automatically handles missing values and supports large datasets.

---

#### 2. Make Predictions

```python
lgbm_pred = lgbm.predict(X_test)
lgbm_proba = lgbm.predict_proba(X_test)[:, 1]
```

- `lgbm_pred`: Predicted class labels for test data.
- `lgbm_proba`: Probabilities of the positive class — used for ROC AUC and ROC curve.

---

#### 3. Evaluate the Model

```python
print(confusion_matrix(y_test, lgbm_pred))
print(classification_report(y_test, lgbm_pred))
print("Accuracy:", accuracy_score(y_test, lgbm_pred))
print("ROC AUC:", roc_auc_score(y_test, lgbm_proba))
```

- **Confusion Matrix**: Breakdown of predicted vs actual.
- **Classification Report**: Includes precision, recall, F1-score.
- **Accuracy**: Percentage of correct predictions.
- **ROC AUC**: Key metric for classifier performance (closer to 1 is better).

---

#### 4. Plot the ROC Curve

```python
fpr, tpr, _ = roc_curve(y_test, lgbm_proba)
plt.plot(fpr, tpr, label="LightGBM (AUC = {:.2f})".format(roc_auc_score(y_test, lgbm_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - LightGBM")
plt.legend()
plt.grid()
plt.show()
```

- The ROC curve visually assesses the model’s classification skill.
- AUC reflects the trade-off between true positive rate and false positive rate.

---

#### 5. Save the Model

```python
joblib.dump(lgbm, "model_lgbm.joblib")
```

- Serializes the trained model to a file for future reuse.

---

 **LightGBM** is known for:
- Fast training speed
- Lower memory usage
- Accuracy that often matches or exceeds XGBoost
- Efficient handling of categorical features

## 9. Support Vector Machine (SVM)
"""

svm = SVC(probability=True)
svm.fit(X_train, y_train)
svm_pred = svm.predict(X_test)
svm_proba = svm.predict_proba(X_test)[:, 1]

print("\nSupport Vector Machine:\n")
print(confusion_matrix(y_test, svm_pred))
print(classification_report(y_test, svm_pred))
print("Accuracy:", accuracy_score(y_test, svm_pred))
print("ROC AUC:", roc_auc_score(y_test, svm_proba))

fpr, tpr, _ = roc_curve(y_test, svm_proba)
plt.plot(fpr, tpr, label="SVM (AUC = {:.2f})".format(roc_auc_score(y_test, svm_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - SVM")
plt.legend()
plt.grid()
plt.show()

joblib.dump(svm, "model_svm.joblib")

"""###  Support Vector Machine (SVM): Training, Evaluation & ROC Analysis

This section implements a **Support Vector Machine (SVM)** classifier — a powerful supervised learning algorithm that seeks the optimal hyperplane to separate classes.

---

#### 1. Initialize and Train the Model

```python
svm = SVC(probability=True)
svm.fit(X_train, y_train)
```

- `SVC(probability=True)` enables probability estimates via Platt scaling (required for AUC & ROC).
- SVM finds the decision boundary (hyperplane) with maximum margin between classes.

---

#### 2. Make Predictions

```python
svm_pred = svm.predict(X_test)
svm_proba = svm.predict_proba(X_test)[:, 1]
```

- `svm_pred`: Class label predictions for test data.
- `svm_proba`: Estimated probability of positive class — used for ROC AUC.

---

#### 3. Evaluate the Model

```python
print(confusion_matrix(y_test, svm_pred))
print(classification_report(y_test, svm_pred))
print("Accuracy:", accuracy_score(y_test, svm_pred))
print("ROC AUC:", roc_auc_score(y_test, svm_proba))
```

- **Confusion Matrix**: Shows correct and incorrect classifications.
- **Classification Report**: Precision, recall, F1-score.
- **Accuracy**: Overall performance metric.
- **ROC AUC**: How well the model distinguishes between classes.

---

#### 4. Plot the ROC Curve

```python
fpr, tpr, _ = roc_curve(y_test, svm_proba)
plt.plot(fpr, tpr, label="SVM (AUC = {:.2f})".format(roc_auc_score(y_test, svm_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - SVM")
plt.legend()
plt.grid()
plt.show()
```

- ROC Curve visualizes model performance across thresholds.
- AUC closer to 1 indicates strong classifier.

---

#### 5. Save the Model

```python
joblib.dump(svm, "model_svm.joblib")
```

- Saves the trained SVM model for future predictions or deployment.

---

 **Notes on SVM**:
- Effective in high-dimensional spaces.
- Sensitive to feature scaling — standardization improves performance.
- Can be slow on large datasets.

## 10. MLP Classifier (Neural Network)
"""

mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)
mlp.fit(X_train, y_train)
mlp_pred = mlp.predict(X_test)
mlp_proba = mlp.predict_proba(X_test)[:, 1]

print("\nMLP Classifier (Neural Network):\n")
print(confusion_matrix(y_test, mlp_pred))
print(classification_report(y_test, mlp_pred))
print("Accuracy:", accuracy_score(y_test, mlp_pred))
print("ROC AUC:", roc_auc_score(y_test, mlp_proba))

fpr, tpr, _ = roc_curve(y_test, mlp_proba)
plt.plot(fpr, tpr, label="MLP Classifier (AUC = {:.2f})".format(roc_auc_score(y_test, mlp_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - MLP Classifier")
plt.legend()
plt.grid()
plt.show()

joblib.dump(mlp, "model_mlp.joblib")

"""###  MLP Classifier (Neural Network): Training, Evaluation & ROC Analysis

This section applies a **Multilayer Perceptron (MLP)** classifier — a type of feedforward artificial neural network trained using backpropagation.

---

#### 1. Initialize and Train the Model

```python
mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)
mlp.fit(X_train, y_train)
```

- `hidden_layer_sizes=(100,)`: One hidden layer with 100 neurons.
- `max_iter=500`: Limits training to 500 iterations.
- `random_state=42`: Ensures reproducibility.

---

#### 2. Make Predictions

```python
mlp_pred = mlp.predict(X_test)
mlp_proba = mlp.predict_proba(X_test)[:, 1]
```

- `mlp_pred`: Class labels (0 or 1).
- `mlp_proba`: Probability scores for positive class (used for ROC and AUC).

---

#### 3. Evaluate the Model

```python
print(confusion_matrix(y_test, mlp_pred))
print(classification_report(y_test, mlp_pred))
print("Accuracy:", accuracy_score(y_test, mlp_pred))
print("ROC AUC:", roc_auc_score(y_test, mlp_proba))
```

- **Confusion Matrix**: Shows TP, TN, FP, FN.
- **Classification Report**: Precision, recall, F1-score per class.
- **Accuracy**: Correct predictions ratio.
- **ROC AUC**: Measures ability to distinguish between classes.

---

#### 4. Plot the ROC Curve

```python
fpr, tpr, _ = roc_curve(y_test, mlp_proba)
plt.plot(fpr, tpr, label="MLP Classifier (AUC = {:.2f})".format(roc_auc_score(y_test, mlp_proba)))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve - MLP Classifier")
plt.legend()
plt.grid()
plt.show()
```

- Visualizes trade-off between sensitivity and specificity.
- AUC closer to 1 = better performance.

---

#### 5. Save the Model

```python
joblib.dump(mlp, "model_mlp.joblib")
```

- Saves the trained MLP model to disk.

---

 **Key Notes on MLP**:
- Requires feature scaling for best performance.
- Suitable for both linear and non-linear classification tasks.
- May need tuning of learning rate, activation functions, or layer sizes.

# Step 15: Comparision Dictionary
"""

# Store model names and their metrics in a dictionary
model_metrics = {
    "Logistic Regression": [accuracy_score(y_test, logistic_pred), precision_score(y_test, logistic_pred),
                            recall_score(y_test, logistic_pred), f1_score(y_test, logistic_pred),
                            roc_auc_score(y_test, logistic_proba)],

    "Random Forest": [accuracy_score(y_test, rf_pred), precision_score(y_test, rf_pred),
                      recall_score(y_test, rf_pred), f1_score(y_test, rf_pred),
                      roc_auc_score(y_test, rf_proba)],

    "KNN": [accuracy_score(y_test, knn_pred), precision_score(y_test, knn_pred),
            recall_score(y_test, knn_pred), f1_score(y_test, knn_pred),
            roc_auc_score(y_test, knn_proba)],

    "Decision Tree": [accuracy_score(y_test, dt_pred), precision_score(y_test, dt_pred),
                      recall_score(y_test, dt_pred), f1_score(y_test, dt_pred),
                      roc_auc_score(y_test, dt_proba)],

    "AdaBoost": [accuracy_score(y_test, ada_pred), precision_score(y_test, ada_pred),
                 recall_score(y_test, ada_pred), f1_score(y_test, ada_pred),
                 roc_auc_score(y_test, ada_proba)],

    "Gradient Boosting": [accuracy_score(y_test, gb_pred), precision_score(y_test, gb_pred),
                          recall_score(y_test, gb_pred), f1_score(y_test, gb_pred),
                          roc_auc_score(y_test, gb_proba)],

    "XGBoost": [accuracy_score(y_test, xgb_pred), precision_score(y_test, xgb_pred),
                recall_score(y_test, xgb_pred), f1_score(y_test, xgb_pred),
                roc_auc_score(y_test, xgb_proba)],

    "LightGBM": [accuracy_score(y_test, lgbm_pred), precision_score(y_test, lgbm_pred),
                 recall_score(y_test, lgbm_pred), f1_score(y_test, lgbm_pred),
                 roc_auc_score(y_test, lgbm_proba)],

    "SVM": [accuracy_score(y_test, svm_pred), precision_score(y_test, svm_pred),
            recall_score(y_test, svm_pred), f1_score(y_test, svm_pred),
            roc_auc_score(y_test, svm_proba)],

    "MLP Classifier": [accuracy_score(y_test, mlp_pred), precision_score(y_test, mlp_pred),
                       recall_score(y_test, mlp_pred), f1_score(y_test, mlp_pred),
                       roc_auc_score(y_test, mlp_proba)]
}

metrics_df = pd.DataFrame(model_metrics, index=["Accuracy", "Precision", "Recall", "F1 Score", "ROC AUC"]).T
metrics_df = metrics_df.sort_values(by="ROC AUC", ascending=False)  # Sort by AUC

# Display the table
print("📋 Final Comparison Table")
metrics_df

# Plot model performance comparison
metrics_df.plot(kind='bar', figsize=(14, 6))
plt.title("Model Comparison - Performance Metrics")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.xticks(rotation=45)
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

"""# Otptimized Models

### I'll optimize all the machine learning models by performing hyperparameter tuning and feature engineering where applicable.
"""

# Feature engineering
data['Glucose_BMI'] = data['Glucose'] * data['BMI']
data['Age_Insulin'] = data['Age'] * data['Insulin']
data['BP_Glucose'] = data['BloodPressure'] * data['Glucose']

"""### Feature Engineering

In this step, we create **new features** from existing ones to help our machine learning models better capture the relationships in the data. These engineered features may reveal hidden patterns that the original features alone might not express well.

Let’s go through each line:

```python
data['Glucose_BMI'] = data['Glucose'] * data['BMI']
```
- Creates a new feature called **`Glucose_BMI`**.
- It multiplies the `Glucose` level with the `BMI` (Body Mass Index).
- The reasoning is that someone with both high blood sugar and high BMI may have a higher risk of diabetes. This combined feature helps capture such interaction effects.

```python
data['Age_Insulin'] = data['Age'] * data['Insulin']
```
- Adds a new column called **`Age_Insulin`**.
- It’s the product of `Age` and `Insulin` levels.
- This helps model interactions between a person’s age and their insulin production—important factors in diabetes risk.

```python
data['BP_Glucose'] = data['BloodPressure'] * data['Glucose']
```
- Introduces a new feature called **`BP_Glucose`**.
- This combines `BloodPressure` and `Glucose` values.
- The idea is to explore whether the joint effect of high blood pressure and high blood sugar indicates higher risk.

>  These new features are examples of **interaction terms**, and they can help machine learning models learn complex relationships more easily. However, we should evaluate their usefulness during model training and validation.

"""

def evaluate_model(model, name, X_test, y_test):
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:,1]

    print(f"\n{name} Performance:")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    print("ROC AUC:", roc_auc_score(y_test, y_proba))

    # Plot ROC
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    plt.plot(fpr, tpr, label=f"{name} (AUC={roc_auc_score(y_test, y_proba):.2f})")

    return {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1': f1_score(y_test, y_pred),
        'ROC AUC': roc_auc_score(y_test, y_proba)
    }

results = {}

"""### Model Evaluation Function

This function helps us evaluate how well a machine learning model performs on test data. It prints key performance metrics and also prepares data for plotting the ROC curve.

---

####  Function Definition

```python
def evaluate_model(model, name, X_test, y_test):
```
- This defines a function named `evaluate_model`.
- It takes four inputs:
  - `model`: the trained ML model to evaluate.
  - `name`: a string (e.g., "Random Forest") used for labeling output and plots.
  - `X_test`: the test input features.
  - `y_test`: the true labels for the test set.

---

####  Model Predictions

```python
    y_pred = model.predict(X_test)
```
- This line generates predicted labels (0 or 1) for the test data.

```python
    y_proba = model.predict_proba(X_test)[:,1]
```
- Here, we get the **probability estimates** for class 1 (i.e., the probability that the model predicts diabetes).
- `[:,1]` selects the probabilities of the positive class (label `1`), which is important for ROC AUC and probability-based evaluations.

---

####  Printing Performance Metrics

```python
    print(f"\n{name} Performance:")
```
- Displays a heading for the evaluation section, indicating which model is being evaluated.

```python
    print("Accuracy:", accuracy_score(y_test, y_pred))
```
- Accuracy: How often the model's predictions match the true labels.

```python
    print("Precision:", precision_score(y_test, y_pred))
```
- Precision: Of all predicted positives, how many were actually positive? (Helps reduce false positives.)

```python
    print("Recall:", recall_score(y_test, y_pred))
```
- Recall (Sensitivity): Of all actual positives, how many did the model correctly identify? (Helps reduce false negatives.)

```python
    print("F1 Score:", f1_score(y_test, y_pred))
```
- F1 Score: Harmonic mean of precision and recall. Useful when classes are imbalanced.

```python
    print("ROC AUC:", roc_auc_score(y_test, y_proba))
```
- ROC AUC: Measures the model's ability to distinguish between classes at different thresholds. AUC closer to 1 is better.

---

####  Plotting the ROC Curve

```python
    fpr, tpr, _ = roc_curve(y_test, y_proba)
```
- Calculates the False Positive Rate and True Positive Rate at various thresholds.

```python
    plt.plot(fpr, tpr, label=f"{name} (AUC={roc_auc_score(y_test, y_proba):.2f})")
```
- Plots the ROC curve and includes the AUC score in the legend for clarity.

---

#### Returning Results as a Dictionary

```python
    return {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1': f1_score(y_test, y_pred),
        'ROC AUC': roc_auc_score(y_test, y_proba)
    }
```
- Returns all the evaluation metrics as a dictionary for future use (e.g., saving to a results table).

---

####  Initializing Results Storage

```python
results = {}
```
- Initializes an empty dictionary to store the evaluation results for different models.

>  This modular function helps us reuse code to evaluate any classifier in a standardized way, making model comparison easier and more consistent.

## Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

lr_params = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga'],
    'max_iter': [100, 200, 500]
}

lr = LogisticRegression(random_state=42)
lr_grid = GridSearchCV(lr, lr_params, cv=5, scoring='roc_auc', n_jobs=-1)
lr_grid.fit(X_train, y_train)
best_lr = lr_grid.best_estimator_
results['Logistic Regression'] = evaluate_model(best_lr, "Logistic Regression", X_test, y_test)
joblib.dump(best_lr, 'best_lr.joblib')

"""###  Logistic Regression with Hyperparameter Tuning

In this cell, we train a **Logistic Regression** model and use **GridSearchCV** to find the best combination of hyperparameters. We then evaluate the model and save the best one.

---

#### Importing Required Libraries

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
```
- `LogisticRegression`: A basic yet powerful classification algorithm for binary problems like diabetes prediction.
- `GridSearchCV`: A tool to exhaustively search over specified hyperparameter values using cross-validation to find the best model.

---

####  Defining the Hyperparameter Grid

```python
lr_params = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga'],
    'max_iter': [100, 200, 500]
}
```
- `C`: Regularization strength. Smaller values mean stronger regularization.
- `penalty`: Type of regularization — `'l1'` (Lasso) or `'l2'` (Ridge).
- `solver`: Algorithm to use for optimization. `'liblinear'` and `'saga'` support `l1` penalty.
- `max_iter`: Maximum number of iterations for convergence.

This grid allows testing different combinations to find the best fit.

---

####  Creating the Logistic Regression Model

```python
lr = LogisticRegression(random_state=42)
```
- Instantiates a basic logistic regression model.
- `random_state=42` ensures reproducibility.

---

####  Applying Grid Search

```python
lr_grid = GridSearchCV(lr, lr_params, cv=5, scoring='roc_auc', n_jobs=-1)
```
- `lr`: the base model.
- `lr_params`: dictionary of hyperparameters to search over.
- `cv=5`: 5-fold cross-validation is used to evaluate each parameter combination.
- `scoring='roc_auc'`: Model performance is evaluated using the **ROC AUC score**.
- `n_jobs=-1`: Uses all available CPU cores to speed up training.

---

####  Training the Best Model

```python
lr_grid.fit(X_train, y_train)
```
- Trains the model on the training data using all combinations of hyperparameters.
- Picks the best model based on highest ROC AUC during cross-validation.

---

####  Selecting the Best Model

```python
best_lr = lr_grid.best_estimator_
```
- Extracts the model with the best combination of hyperparameters.

---

#### Evaluating the Best Logistic Regression Model

```python
results['Logistic Regression'] = evaluate_model(best_lr, "Logistic Regression", X_test, y_test)
```
- Evaluates the best model using our custom `evaluate_model` function.
- Saves the performance metrics in the `results` dictionary under the key `"Logistic Regression"`.

---

####  Saving the Trained Model

```python
joblib.dump(best_lr, 'best_lr.joblib')
```
- Saves the trained model to a file named `best_lr.joblib`.
- This allows us to load the model later without retraining, which is useful for deployment or further testing.

>  Logistic Regression is often used as a baseline model. Even though it's simple, it can perform surprisingly well, especially when combined with proper feature engineering and hyperparameter tuning.

## Random Forest
"""

from sklearn.ensemble import RandomForestClassifier

rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

rf = RandomForestClassifier(random_state=42)
rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='roc_auc', n_jobs=-1)
rf_grid.fit(X_train, y_train)
best_rf = rf_grid.best_estimator_
results['Random Forest'] = evaluate_model(best_rf, "Random Forest", X_test, y_test)
joblib.dump(best_rf, 'best_rf.joblib')

"""###  Random Forest with Hyperparameter Tuning

In this cell, we train a **Random Forest Classifier** and use **GridSearchCV** to tune its hyperparameters. The best model is evaluated and saved.

---

####  Importing the Classifier

```python
from sklearn.ensemble import RandomForestClassifier
```
- Imports the Random Forest algorithm, an ensemble method that builds multiple decision trees and combines their outputs for better performance and stability.

---

####  Defining the Hyperparameter Grid

```python
rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}
```

- `n_estimators`: Number of trees in the forest. More trees usually improve performance but increase training time.
- `max_depth`: Maximum depth of each tree. `None` means the tree will grow until all leaves are pure.
- `min_samples_split`: Minimum number of samples required to split an internal node.
- `min_samples_leaf`: Minimum number of samples required to be at a leaf node.
- `max_features`: Number of features to consider when looking for the best split:
  - `'sqrt'`: square root of total features (default for classification).
  - `'log2'`: logarithm (base 2) of total features.

This grid covers a variety of model complexities and generalization levels.

---

####  Creating the Base Random Forest Model

```python
rf = RandomForestClassifier(random_state=42)
```
- Instantiates the model.
- `random_state=42` ensures reproducibility by fixing the randomness.

---

####  Performing Grid Search

```python
rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='roc_auc', n_jobs=-1)
```
- Uses 5-fold cross-validation (`cv=5`) to evaluate each parameter combination.
- Evaluates performance using **ROC AUC**.
- `n_jobs=-1` uses all available CPU cores to run faster.

---

#### Training the Best Model

```python
rf_grid.fit(X_train, y_train)
```
- Trains the model on the training data, trying every combination in the parameter grid.
- Cross-validates each combination to find the best one.

---

####  Selecting the Best Estimator

```python
best_rf = rf_grid.best_estimator_
```
- Extracts the trained model with the best-performing parameter combination.

---

####  Evaluating the Model

```python
results['Random Forest'] = evaluate_model(best_rf, "Random Forest", X_test, y_test)
```
- Uses our custom evaluation function to compute and print metrics like accuracy, precision, recall, F1, and ROC AUC.
- Saves the results in the `results` dictionary under the key `"Random Forest"`.

---

#### Saving the Trained Model

```python
joblib.dump(best_rf, 'best_rf.joblib')
```
- Saves the trained model to a file named `'best_rf.joblib'`.
- Useful for deploying or reusing the model without retraining.

>  Random Forests are powerful and work well with both linear and non-linear data. Tuning them carefully can significantly boost predictive performance.

## KNN
"""

from sklearn.neighbors import KNeighborsClassifier

knn_params = {
    'n_neighbors': range(3, 22, 2),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

knn = KNeighborsClassifier()
knn_grid = GridSearchCV(knn, knn_params, cv=5, scoring='roc_auc', n_jobs=-1)
knn_grid.fit(X_train, y_train)
best_knn = knn_grid.best_estimator_
results['KNN'] = evaluate_model(best_knn, "KNN", X_test, y_test)
joblib.dump(best_knn, 'best_knn.joblib')

"""###  K-Nearest Neighbors (KNN) with Hyperparameter Tuning

In this cell, we train a **K-Nearest Neighbors (KNN)** classifier and use **GridSearchCV** to find the best hyperparameters. Then, we evaluate the best model and save it.

---

####  Importing the KNN Classifier

```python
from sklearn.neighbors import KNeighborsClassifier
```
- Imports the KNN algorithm, which classifies a data point based on how its **nearest neighbors** are classified.
- It is a simple and intuitive algorithm, especially useful for smaller datasets.

---

####  Defining the Hyperparameter Grid

```python
knn_params = {
    'n_neighbors': range(3, 22, 2),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}
```

- `n_neighbors`: Number of neighbors to consider. Using odd numbers helps avoid ties. The range from 3 to 21 tests various sizes.
- `weights`:
  - `'uniform'`: All neighbors have equal weight.
  - `'distance'`: Closer neighbors have more influence.
- `metric`: Distance measure to compute closeness between data points.
  - `'euclidean'`: Straight-line distance.
  - `'manhattan'`: Distance by only horizontal and vertical steps (like a grid).

This grid tests multiple combinations to find what works best for the dataset.

---

####  Creating the Base KNN Model

```python
knn = KNeighborsClassifier()
```
- Instantiates the KNN model with default parameters.

---

####  Applying Grid Search

```python
knn_grid = GridSearchCV(knn, knn_params, cv=5, scoring='roc_auc', n_jobs=-1)
```
- `cv=5`: Performs 5-fold cross-validation to evaluate each parameter set.
- `scoring='roc_auc'`: Uses ROC AUC to measure model performance.
- `n_jobs=-1`: Uses all available CPU cores to run faster.

---

####  Training the Grid Search Model

```python
knn_grid.fit(X_train, y_train)
```
- Trains multiple KNN models using the combinations in the parameter grid.
- Finds the best set of hyperparameters using cross-validation performance.

---

####  Selecting the Best Model

```python
best_knn = knn_grid.best_estimator_
```
- Retrieves the best-performing KNN model from grid search.

---

####  Evaluating the Model

```python
results['KNN'] = evaluate_model(best_knn, "KNN", X_test, y_test)
```
- Evaluates the best KNN model on test data.
- Uses the `evaluate_model` function to print and return key metrics.
- Stores the results in the `results` dictionary under `"KNN"`.

---

####  Saving the Trained Model

```python
joblib.dump(best_knn, 'best_knn.joblib')
```
- Saves the trained KNN model to a file for future use without needing retraining.

>  KNN is easy to understand and implement. It's non-parametric, meaning it doesn’t make strong assumptions about the data—but it can be slow on large datasets and sensitive to noisy data.

## Decision Tree
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV  # Faster than GridSearchCV
import numpy as np

# Reduced parameter space with intelligent defaults
rf_params = {
    'n_estimators': [50, 100, 150],  # Reduced from [100, 200, 300]
    'max_depth': [None, 10, 15],      # Reduced from [None, 10, 20, 30]
    'min_samples_split': [2, 5],       # Reduced from [2, 5, 10]
    'min_samples_leaf': [1, 2],        # Reduced from [1, 2, 4]
    'max_features': ['sqrt'],          # Removed 'log2' (sqrt is usually better)
    'bootstrap': [True]                # Added fixed value
}

# Use RandomizedSearchCV instead of GridSearchCV
rf = RandomForestClassifier(random_state=42, n_jobs=-1)  # Parallelize the forest
rf_search = RandomizedSearchCV(
    rf,
    rf_params,
    n_iter=15,           # Number of parameter combinations to try
    cv=3,                # Reduced from 5-fold
    scoring='roc_auc',
    n_jobs=-1,           # Use all cores
    random_state=42
)

rf_search.fit(X_train, y_train)
best_rf = rf_search.best_estimator_

print("Best Random Forest Parameters:")
print(rf_search.best_params_)
results['Random Forest'] = evaluate_model(best_rf, "Random Forest", X_test, y_test)
joblib.dump(best_rf, 'optimized_rf.joblib')

"""### Optimized Random Forest using RandomizedSearchCV

In this cell, we train a **Random Forest Classifier** using **RandomizedSearchCV** instead of GridSearchCV for faster hyperparameter tuning. We reduce the parameter space and focus on smart defaults to save time while maintaining performance.

---

####  Importing Required Libraries

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
import numpy as np
```
- `RandomForestClassifier`: Ensemble method that builds multiple decision trees and averages their predictions.
- `RandomizedSearchCV`: A faster alternative to GridSearchCV that samples from the hyperparameter space randomly.
- `numpy`: Required internally by `RandomizedSearchCV`.

---

####  Defining a Simplified Hyperparameter Search Space

```python
rf_params = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 15],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt'],
    'bootstrap': [True]
}
```

We reduce the size of the search space to save time:
- `n_estimators`: Number of trees in the forest.
- `max_depth`: Maximum depth of each tree.
- `min_samples_split`: Minimum samples required to split an internal node.
- `min_samples_leaf`: Minimum samples required to be at a leaf node.
- `max_features`: We stick to `'sqrt'` which usually works well.
- `bootstrap`: Controls whether bootstrap samples are used when building trees. We fix this to `True`.

> Smaller, focused search spaces often yield good results with much less computation.

---

####  Creating the Base Random Forest Model

```python
rf = RandomForestClassifier(random_state=42, n_jobs=-1)
```
- `random_state=42`: Ensures reproducibility.
- `n_jobs=-1`: Enables parallel training using all available CPU cores.

---

####  Setting Up Randomized Search

```python
rf_search = RandomizedSearchCV(
    rf,
    rf_params,
    n_iter=15,
    cv=3,
    scoring='roc_auc',
    n_jobs=-1,
    random_state=42
)
```
- `n_iter=15`: Only 15 random combinations are tried (compared to GridSearchCV which tries all combinations).
- `cv=3`: Uses 3-fold cross-validation to speed things up.
- `scoring='roc_auc'`: Evaluates models based on ROC AUC, a good metric for imbalanced classification tasks.
- `random_state`: Ensures reproducibility.
- `n_jobs=-1`: Uses all CPU cores for parallel processing.

---

####  Fitting the Model

```python
rf_search.fit(X_train, y_train)
```
- Trains the Random Forest on the training data using the random search over hyperparameters.

---

#### Selecting and Displaying the Best Model

```python
best_rf = rf_search.best_estimator_
print("Best Random Forest Parameters:")
print(rf_search.best_params_)
```
- Extracts the best-performing model from the search.
- Prints the combination of hyperparameters that worked best.

---

####  Evaluating the Best Model

```python
results['Random Forest'] = evaluate_model(best_rf, "Random Forest", X_test, y_test)
```
- Uses the `evaluate_model` function to compute and print metrics like accuracy, precision, recall, F1-score, and ROC AUC.
- Saves the metrics in the `results` dictionary.

---

####  Saving the Model

```python
joblib.dump(best_rf, 'optimized_rf.joblib')
```
- Saves the best model to a file for later use, eliminating the need for retraining.

>  `RandomizedSearchCV` is great when you want to balance performance with speed. It gives near-optimal results with much lower computational cost compared to an exhaustive grid search.

## AdaBoost
"""

from sklearn.ensemble import AdaBoostClassifier

ada_params = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.5, 1.0],
    'algorithm': ['SAMME', 'SAMME.R']
}

ada = AdaBoostClassifier(random_state=42)
ada_grid = GridSearchCV(ada, ada_params, cv=5, scoring='roc_auc', n_jobs=-1)
ada_grid.fit(X_train, y_train)
best_ada = ada_grid.best_estimator_
results['AdaBoost'] = evaluate_model(best_ada, "AdaBoost", X_test, y_test)
joblib.dump(best_ada, 'best_ada.joblib')

"""### AdaBoost Classifier with GridSearchCV

In this cell, we train an **AdaBoost (Adaptive Boosting)** classifier using **GridSearchCV** to tune its hyperparameters. The best model is then evaluated and saved.

---

####  Importing the Classifier

```python
from sklearn.ensemble import AdaBoostClassifier
```
- Imports the AdaBoost algorithm, which combines many weak learners (usually decision trees) into a strong ensemble.
- Boosting works by focusing more on instances the previous models got wrong.

---

####  Defining the Hyperparameter Grid

```python
ada_params = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.5, 1.0],
    'algorithm': ['SAMME', 'SAMME.R']
}
```

- `n_estimators`: Number of weak learners (usually small decision trees) to be used. More learners can improve performance but increase computation.
- `learning_rate`: Controls how much each learner contributes. Lower values make learning slower but potentially more accurate.
- `algorithm`:
  - `'SAMME'`: Uses class labels only.
  - `'SAMME.R'`: Uses predicted probabilities; typically faster and more accurate.

>  These parameters allow us to control the model complexity and learning dynamics.

---

#### Creating the Base AdaBoost Model

```python
ada = AdaBoostClassifier(random_state=42)
```
- Initializes the AdaBoost model with a fixed random state for reproducibility.

---

####  Performing Grid Search for Hyperparameter Tuning

```python
ada_grid = GridSearchCV(ada, ada_params, cv=5, scoring='roc_auc', n_jobs=-1)
```

- `GridSearchCV`: Tries all combinations of hyperparameters using cross-validation.
- `cv=5`: Uses 5-fold cross-validation to estimate model performance.
- `scoring='roc_auc'`: Evaluates models using ROC AUC score.
- `n_jobs=-1`: Uses all CPU cores to train models in parallel.

---

####  Training the Grid Search Model

```python
ada_grid.fit(X_train, y_train)
```
- Fits the model on training data for each hyperparameter combination.
- Cross-validation is used to select the best model.

---

####  Extracting the Best Model

```python
best_ada = ada_grid.best_estimator_
```
- Retrieves the AdaBoost model that had the best ROC AUC performance during cross-validation.

---

####  Evaluating the Best Model

```python
results['AdaBoost'] = evaluate_model(best_ada, "AdaBoost", X_test, y_test)
```
- Evaluates the best AdaBoost model on the test set using custom metrics like accuracy, precision, recall, F1-score, and ROC AUC.
- Stores the results in the `results` dictionary under the key `'AdaBoost'`.

---

####  Saving the Model

```python
joblib.dump(best_ada, 'best_ada.joblib')
```
- Saves the trained AdaBoost model to a file so it can be reloaded later for predictions or deployment.

>  AdaBoost works well when base learners are slightly better than random guessing, and it's robust to overfitting in many cases.

## Gradient Boosting
"""

# Optimized Gradient Boosting with faster hyperparameter tuning
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV


# Reduced parameter space focusing on most impactful parameters
gb_params = {
    'n_estimators': np.arange(50, 151, 50),  # 50, 100, 150 (smaller range)
    'learning_rate': [0.05, 0.1],           # Most critical parameter
    'max_depth': [3, 4],                    # Shallower trees (faster)
    'min_samples_split': [2, 5],            # Only test key splits
    'subsample': [0.8, 1.0]                 # Stochastic boosting for speed
}

# Initialize with some speed optimizations
gb = GradientBoostingClassifier(
    random_state=42,
    validation_fraction=0.1,  # Internal early stopping
    n_iter_no_change=5,       # Stop if no improvement
    tol=1e-4                  # Slightly relaxed tolerance
)

# Randomized search is much faster than grid search
gb_search = RandomizedSearchCV(
    gb,
    gb_params,
    n_iter=10,                # Test only 10 combinations (instead of all)
    cv=3,                     # Reduced from 5-fold to 3-fold
    scoring='roc_auc',
    n_jobs=-1,                # Use all cores
    random_state=42
)

# Train with progress monitoring
print("Starting optimized Gradient Boosting tuning...")
gb_search.fit(X_train, y_train)
print("Tuning completed!")

# Get best model
best_gb = gb_search.best_estimator_
print("\nBest Parameters Found:")
print(gb_search.best_params_)

# Evaluate and save
results['Gradient Boosting'] = evaluate_model(best_gb, "Gradient Boosting", X_test, y_test)
joblib.dump(best_gb, 'optimized_gb.joblib')

"""###  Optimized Gradient Boosting with RandomizedSearchCV

In this cell, we train a **Gradient Boosting Classifier** using an optimized and efficient hyperparameter tuning approach with `RandomizedSearchCV`. This version focuses on speed while still exploring key parameters to maximize performance.

---

####  Import Required Libraries

```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV
```

- `GradientBoostingClassifier`: A powerful ensemble learning method that builds trees sequentially, where each new tree corrects the errors of the previous one.
- `RandomizedSearchCV`: Faster alternative to `GridSearchCV`, which samples a fixed number of random combinations from the hyperparameter space.

---

####  Define a Reduced & Focused Parameter Grid

```python
gb_params = {
    'n_estimators': np.arange(50, 151, 50),
    'learning_rate': [0.05, 0.1],
    'max_depth': [3, 4],
    'min_samples_split': [2, 5],
    'subsample': [0.8, 1.0]
}
```

- `n_estimators`: Number of boosting stages (trees). Fewer values (50, 100, 150) for faster tuning.
- `learning_rate`: Shrinks the contribution of each tree. Lower values can improve performance with more trees.
- `max_depth`: Depth of each tree. Shallow trees (3 or 4) train faster and reduce overfitting.
- `min_samples_split`: Minimum number of samples required to split an internal node.
- `subsample`: Fraction of samples used to train each tree. Less than 1.0 introduces randomness (stochastic boosting), often improving generalization and speed.

>  This search space is carefully chosen to strike a balance between model quality and tuning efficiency.

---

####  Initialize the Gradient Boosting Classifier with Early Stopping

```python
gb = GradientBoostingClassifier(
    random_state=42,
    validation_fraction=0.1,
    n_iter_no_change=5,
    tol=1e-4
)
```

- `validation_fraction=0.1`: Internally holds out 10% of the training data for early stopping.
- `n_iter_no_change=5`: Training will stop if there is no improvement in 5 iterations.
- `tol=1e-4`: Tolerance for deciding when to stop early. A slightly relaxed value to stop faster.
- These settings help reduce unnecessary training time and avoid overfitting.

---

####  Set Up Randomized Hyperparameter Search

```python
gb_search = RandomizedSearchCV(
    gb,
    gb_params,
    n_iter=10,
    cv=3,
    scoring='roc_auc',
    n_jobs=-1,
    random_state=42
)
```

- `n_iter=10`: Only 10 random combinations are tested, saving time.
- `cv=3`: 3-fold cross-validation is faster than the usual 5.
- `scoring='roc_auc'`: Optimizes the area under the ROC curve — a good metric for classification problems.
- `n_jobs=-1`: Uses all available CPU cores for parallel training.

---

####  Train the Model with Progress Output

```python
print("Starting optimized Gradient Boosting tuning...")
gb_search.fit(X_train, y_train)
print("Tuning completed!")
```

- Fits the model using cross-validation and shows progress to the user.

---

####  Extract and Display the Best Model

```python
best_gb = gb_search.best_estimator_
print("\nBest Parameters Found:")
print(gb_search.best_params_)
```

- Gets the best model based on ROC AUC score and prints the optimal hyperparameters found.

---

#### Evaluate and Save the Model

```python
results['Gradient Boosting'] = evaluate_model(best_gb, "Gradient Boosting", X_test, y_test)
joblib.dump(best_gb, 'optimized_gb.joblib')
```

- Evaluates the model using metrics like accuracy, precision, recall, F1-score, and ROC AUC.
- Saves the best model using `joblib` for future use or deployment.

>  Gradient Boosting is a strong performer on structured/tabular data, and this optimized setup makes it efficient to tune and train.

## XGBoost
"""

from xgboost import XGBClassifier

xgb_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

xgb = XGBClassifier(random_state=42, eval_metric='logloss')
xgb_grid = GridSearchCV(xgb, xgb_params, cv=5, scoring='roc_auc', n_jobs=-1)
xgb_grid.fit(X_train, y_train)
best_xgb = xgb_grid.best_estimator_
results['XGBoost'] = evaluate_model(best_xgb, "XGBoost", X_test, y_test)
joblib.dump(best_xgb, 'best_xgb.joblib')

"""###  XGBoost Classifier with GridSearchCV

In this cell, we train an **XGBoost (Extreme Gradient Boosting)** classifier, a popular and powerful machine learning algorithm, using **GridSearchCV** for hyperparameter tuning to find the optimal model.

---

#### Importing the XGBoost Classifier

```python
from xgboost import XGBClassifier
```
- Imports `XGBClassifier` from the `xgboost` library. XGBoost is known for its speed and accuracy and is one of the most widely used algorithms in Kaggle competitions.

---

####  Define the Hyperparameter Grid

```python
xgb_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}
```

- `n_estimators`: Number of boosting rounds or trees. More trees often lead to better performance but higher computation.
- `max_depth`: Maximum depth of the trees. Larger values allow the model to capture more complex patterns but can lead to overfitting.
- `learning_rate`: Shrinks the contribution of each tree. A smaller rate (e.g., 0.01) requires more trees for the same effect.
- `subsample`: Fraction of the training data to randomly sample for each tree. Reduces overfitting when set to less than 1.0.
- `colsample_bytree`: Fraction of features to use for each tree. Reducing this can help prevent overfitting and improve speed.

>  These hyperparameters are crucial for balancing model complexity, training time, and generalization ability.

---

####  Initialize the XGBoost Model

```python
xgb = XGBClassifier(random_state=42, eval_metric='logloss')
```

- `random_state=42`: Ensures reproducibility by fixing the random seed.
- `eval_metric='logloss'`: Logarithmic loss is used for evaluation during training, which is appropriate for binary classification tasks.

---

####  Perform Grid Search for Hyperparameter Tuning

```python
xgb_grid = GridSearchCV(xgb, xgb_params, cv=5, scoring='roc_auc', n_jobs=-1)
```

- `GridSearchCV`: Tries every combination of the parameters defined in `xgb_params` and evaluates the model using cross-validation.
- `cv=5`: 5-fold cross-validation is used to evaluate model performance on different splits of the data.
- `scoring='roc_auc'`: Uses the ROC AUC score to evaluate model performance. ROC AUC is a good metric for imbalanced classes.
- `n_jobs=-1`: Uses all available CPU cores to parallelize the grid search process, speeding up computation.

---

####  Train the Model with Grid Search

```python
xgb_grid.fit(X_train, y_train)
```

- Fits the XGBoost model on the training data for each hyperparameter combination, using cross-validation to estimate performance.

---

####  Extract the Best Model

```python
best_xgb = xgb_grid.best_estimator_
```

- Retrieves the best XGBoost model based on the ROC AUC score obtained during cross-validation.

---

####  Evaluate the Best Model

```python
results['XGBoost'] = evaluate_model(best_xgb, "XGBoost", X_test, y_test)
```

- Evaluates the performance of the best XGBoost model on the test set using various classification metrics like accuracy, precision, recall, F1-score, and ROC AUC.

---

####  Save the Best Model

```python
joblib.dump(best_xgb, 'best_xgb.joblib')
```

- Saves the best XGBoost model to a file using `joblib`, so it can be reloaded later for prediction or deployment.

>  XGBoost is often the go-to algorithm for many machine learning tasks, as it combines speed and accuracy with excellent performance on structured/tabular data.

## LightGBM
"""

from lightgbm import LGBMClassifier

lgbm_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [-1, 5, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'num_leaves': [31, 50, 100],
    'min_child_samples': [20, 50]
}

lgbm = LGBMClassifier(random_state=42)
lgbm_grid = GridSearchCV(lgbm, lgbm_params, cv=5, scoring='roc_auc', n_jobs=-1)
lgbm_grid.fit(X_train, y_train)
best_lgbm = lgbm_grid.best_estimator_
results['LightGBM'] = evaluate_model(best_lgbm, "LightGBM", X_test, y_test)
joblib.dump(best_lgbm, 'best_lgbm.joblib')

"""###  LightGBM Classifier with GridSearchCV

In this cell, we train a **LightGBM (Light Gradient Boosting Machine)** classifier, a popular and efficient gradient boosting framework, using **GridSearchCV** to tune hyperparameters and identify the best model.

---

####  Importing the LightGBM Classifier

```python
from lightgbm import LGBMClassifier
```

- Imports `LGBMClassifier` from the `lightgbm` library. LightGBM is known for being fast and scalable, often outperforming other gradient boosting algorithms in terms of speed and memory efficiency.

---

####  Define the Hyperparameter Grid

```python
lgbm_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [-1, 5, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'num_leaves': [31, 50, 100],
    'min_child_samples': [20, 50]
}
```

- `n_estimators`: Number of boosting iterations (trees). More trees can improve performance, but they increase training time.
- `max_depth`: Maximum depth of each tree. `-1` means no limit. Higher values increase model complexity and may lead to overfitting.
- `learning_rate`: Controls the contribution of each tree. A lower learning rate requires more trees for the same effect.
- `num_leaves`: The number of leaves in each tree. More leaves allow for more complex models but increase computation.
- `min_child_samples`: Minimum number of samples in a child node. This parameter helps control overfitting by reducing the tree's complexity.

>  The choice of hyperparameters is aimed at controlling both model complexity and training time, balancing performance with efficiency.

---

####  Initialize the LightGBM Model

```python
lgbm = LGBMClassifier(random_state=42)
```

- `random_state=42`: Ensures reproducibility by fixing the random seed.
- LightGBM is a powerful boosting method, and this initialization prepares it to be tuned for optimal performance.

---

####  Perform Grid Search for Hyperparameter Tuning

```python
lgbm_grid = GridSearchCV(lgbm, lgbm_params, cv=5, scoring='roc_auc', n_jobs=-1)
```

- `GridSearchCV`: Searches over all hyperparameter combinations defined in `lgbm_params` and evaluates the models using cross-validation.
- `cv=5`: 5-fold cross-validation splits the data into 5 parts, training the model on 4 and testing it on the remaining 1, cycling through each fold.
- `scoring='roc_auc'`: Optimizes the model for the area under the ROC curve, a good metric for classification tasks.
- `n_jobs=-1`: Uses all available CPU cores for parallelized computation, speeding up the grid search process.

---

####  Train the Model with Grid Search

```python
lgbm_grid.fit(X_train, y_train)
```

- Fits the LightGBM model on the training data for each combination of hyperparameters defined in `lgbm_params`. It performs cross-validation to estimate the performance of each combination.

---

#### Extract the Best Model

```python
best_lgbm = lgbm_grid.best_estimator_
```

- Retrieves the best LightGBM model based on the highest ROC AUC score obtained during cross-validation.

---

#### Evaluate the Best Model

```python
results['LightGBM'] = evaluate_model(best_lgbm, "LightGBM", X_test, y_test)
```

- Evaluates the best LightGBM model on the test data using multiple classification metrics such as accuracy, precision, recall, F1-score, and ROC AUC.

---

#### Save the Best Model

```python
joblib.dump(best_lgbm, 'best_lgbm.joblib')
```

- Saves the best LightGBM model to a file using `joblib`. This allows for easy reloading and use of the model later, without retraining it.

>  LightGBM is highly efficient and often delivers excellent results, especially with large datasets, by focusing on speed and low memory usage.

## SVM
"""

from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV  # Faster than GridSearchCV
import joblib

# Optimized parameter space
svm_params = {
    'C': [0.1, 1, 10],  # Reduced from [0.1, 1, 10, 100]
    'kernel': ['linear', 'rbf'],  # Removed 'poly' which is often slower
    'gamma': ['scale'],  # Fixed to 'scale' (usually better than 'auto')
    'probability': [True],
    'cache_size': [1000]  # Added for speed
}

# Initialize with speed optimizations
svm = SVC(
    random_state=42,
    shrinking=True,  # Helps with speed
    tol=0.001,  # Slightly relaxed tolerance
    max_iter=1000  # Limit iterations
)

# Use RandomizedSearchCV with fewer iterations
svm_search = RandomizedSearchCV(
    svm,
    svm_params,
    n_iter=8,  # Test only 8 combinations (originally 3*2*2=12)
    cv=3,  # Reduced from 5-fold
    scoring='roc_auc',
    n_jobs=-1,  # Use all cores
    random_state=42
)

# Train with progress monitoring
print("Starting optimized SVM tuning...")
svm_search.fit(X_train, y_train)
print("Tuning completed!")

# Get best model
best_svm = svm_search.best_estimator_
print("\nBest Parameters Found:")
print(svm_search.best_params_)

# Evaluate and save
results['SVM'] = evaluate_model(best_svm, "SVM", X_test, y_test)
joblib.dump(best_svm, 'optimized_svm.joblib')

"""###  Support Vector Machine (SVM) Classifier with RandomizedSearchCV

In this cell, we train a **Support Vector Machine (SVM)** classifier using **RandomizedSearchCV** to optimize its hyperparameters, aiming to find the best performing model.

---

####  Importing the SVM Classifier

```python
from sklearn.svm import SVC
```

- Imports the `SVC` (Support Vector Classification) class from the `sklearn.svm` module. SVM is a powerful classification algorithm that works well for both linear and non-linear decision boundaries.

---

####  Define the Hyperparameter Grid

```python
svm_params = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale'],
    'probability': [True],
    'cache_size': [1000]
}
```

- `C`: Regularization parameter. Controls the trade-off between achieving a low error on the training data and minimizing the model's complexity. Larger values of `C` lead to a model that fits the training data more closely.
- `kernel`: Specifies the type of kernel used in the SVM. The `linear` kernel is for linear decision boundaries, while the `rbf` (Radial Basis Function) kernel is used for non-linear decision boundaries.
- `gamma`: Defines how far the influence of a single training example reaches. `scale` (default) is generally preferred over `auto`, as it is more effective for most cases.
- `probability`: If set to `True`, enables probability estimates (calculated using Platt scaling).
- `cache_size`: Increases the cache size to improve computation speed during training. A larger value can speed up training, especially on large datasets.

>  This hyperparameter space focuses on optimizing the model's flexibility and speed while ensuring it's efficient in both computation and memory usage.

---

####  Initialize the SVM Model

```python
svm = SVC(
    random_state=42,
    shrinking=True,
    tol=0.001,
    max_iter=1000
)
```

- `random_state=42`: Ensures reproducibility by fixing the random seed.
- `shrinking=True`: Speeds up the algorithm by simplifying the solution when the optimization problem is solved.
- `tol=0.001`: Specifies the tolerance for stopping criteria. A smaller value increases the accuracy but also the training time.
- `max_iter=1000`: Limits the maximum number of iterations to prevent excessively long training times.

---

####  Perform Randomized Search for Hyperparameter Tuning

```python
svm_search = RandomizedSearchCV(
    svm,
    svm_params,
    n_iter=8,
    cv=3,
    scoring='roc_auc',
    n_jobs=-1,
    random_state=42
)
```

- `RandomizedSearchCV`: A faster alternative to `GridSearchCV` that randomly samples a specified number of hyperparameter combinations. This helps find good hyperparameters more quickly by testing fewer combinations.
- `n_iter=8`: Limits the number of parameter combinations to 8, reducing the search space and thus the computation time.
- `cv=3`: Uses 3-fold cross-validation instead of the typical 5-fold to speed up the process while still evaluating performance on different data splits.
- `scoring='roc_auc'`: Optimizes the model for the ROC AUC score, which is suitable for imbalanced classes and provides insights into model performance across all thresholds.
- `n_jobs=-1`: Uses all available CPU cores for parallel processing, speeding up the search process.

---

####  Train the Model with Randomized Search

```python
svm_search.fit(X_train, y_train)
```

- Fits the SVM model on the training data for each combination of hyperparameters defined in `svm_params`. This process uses cross-validation to evaluate each hyperparameter combination's performance.

---

####  Extract the Best Model

```python
best_svm = svm_search.best_estimator_
```

- Retrieves the best SVM model based on the highest ROC AUC score obtained during cross-validation.

---

####  Evaluate the Best Model

```python
results['SVM'] = evaluate_model(best_svm, "SVM", X_test, y_test)
```

- Evaluates the best SVM model on the test data, using a variety of classification metrics such as accuracy, precision, recall, F1-score, and ROC AUC to assess performance.

---

####  Save the Best Model

```python
joblib.dump(best_svm, 'optimized_svm.joblib')
```

- Saves the best SVM model to a file using `joblib`, allowing it to be reloaded for future predictions or deployment without needing to retrain the model.

>  Support Vector Machines are highly effective in high-dimensional spaces and are particularly well-suited for classification tasks with clear margins of separation.

## Neural Network (MLP)
"""

from sklearn.neural_network import MLPClassifier

mlp_params = {
    'hidden_layer_sizes': [(50,), (100,), (50,50)],
    'activation': ['relu', 'tanh'],
    'alpha': [0.0001, 0.001, 0.01],
    'learning_rate': ['constant', 'adaptive'],
    'max_iter': [500]
}

mlp = MLPClassifier(random_state=42)
mlp_grid = GridSearchCV(mlp, mlp_params, cv=5, scoring='roc_auc', n_jobs=-1)
mlp_grid.fit(X_train, y_train)
best_mlp = mlp_grid.best_estimator_
results['Neural Network'] = evaluate_model(best_mlp, "Neural Network", X_test, y_test)
joblib.dump(best_mlp, 'best_mlp.joblib')

"""###  Multi-Layer Perceptron (MLP) Classifier with GridSearchCV

In this cell, we train a **Multi-Layer Perceptron (MLP)** classifier using **GridSearchCV** to optimize its hyperparameters and find the best-performing model.

---

####  Importing the MLP Classifier

```python
from sklearn.neural_network import MLPClassifier
```

- Imports the `MLPClassifier` class from the `sklearn.neural_network` module. An MLP is a type of artificial neural network used for classification tasks. It consists of multiple layers of neurons (input, hidden, and output layers) and is trained using backpropagation.

---

####  Define the Hyperparameter Grid

```python
mlp_params = {
    'hidden_layer_sizes': [(50,), (100,), (50,50)],
    'activation': ['relu', 'tanh'],
    'alpha': [0.0001, 0.001, 0.01],
    'learning_rate': ['constant', 'adaptive'],
    'max_iter': [500]
}
```

- `hidden_layer_sizes`: Defines the number of neurons in each hidden layer. For example, `(50,)` means a network with one hidden layer of 50 neurons, and `(50,50)` means two hidden layers, each with 50 neurons. Experimenting with different architectures helps to balance performance and complexity.
- `activation`: The activation function for the hidden layers.
  - `'relu'` (Rectified Linear Unit) is widely used and helps in faster training by avoiding the vanishing gradient problem.
  - `'tanh'` (Hyperbolic Tangent) produces outputs between -1 and 1 but is less common in deep networks due to its tendency to saturate during training.
- `alpha`: Regularization term (L2 penalty) that controls the strength of regularization. Higher values apply stronger regularization, reducing the model's complexity.
- `learning_rate`: Controls how fast the model learns.
  - `'constant'` means the learning rate remains fixed throughout training.
  - `'adaptive'` means the learning rate will decrease when the model stops improving, which can help fine-tune the model.
- `max_iter`: Limits the number of iterations (epochs) for training. A higher number may improve accuracy, but it can also lead to overfitting and longer training times.

>  The hyperparameter grid is designed to explore the model's architecture, regularization strength, learning behavior, and optimization speed.

---

####  Initialize the MLP Classifier

```python
mlp = MLPClassifier(random_state=42)
```

- `random_state=42`: Ensures that the training process is reproducible by fixing the random seed.
- By default, `MLPClassifier` uses the `adam` optimizer, which is an efficient optimization method for training neural networks.

---

####  Perform Grid Search for Hyperparameter Tuning

```python
mlp_grid = GridSearchCV(mlp, mlp_params, cv=5, scoring='roc_auc', n_jobs=-1)
```

- `GridSearchCV`: Exhaustively tests all possible combinations of the hyperparameters defined in `mlp_params`. It performs cross-validation to find the hyperparameter combination that yields the best performance.
- `cv=5`: Uses 5-fold cross-validation, which splits the data into 5 subsets and trains the model 5 times, each time using a different subset as the validation set.
- `scoring='roc_auc'`: Optimizes the model for the ROC AUC score, which evaluates classification performance for imbalanced datasets.
- `n_jobs=-1`: Utilizes all available CPU cores to speed up the grid search process.

---

####  Train the Model with Grid Search

```python
mlp_grid.fit(X_train, y_train)
```

- Fits the MLP model using all combinations of the hyperparameters in `mlp_params` and evaluates them using cross-validation.

---

####  Extract the Best Model

```python
best_mlp = mlp_grid.best_estimator_
```

- Retrieves the MLP model with the best-performing hyperparameters, based on the highest ROC AUC score achieved during cross-validation.

---

####  Evaluate the Best Model

```python
results['Neural Network'] = evaluate_model(best_mlp, "Neural Network", X_test, y_test)
```

- Evaluates the best MLP model on the test data, calculating a range of classification metrics such as accuracy, precision, recall, F1-score, and ROC AUC.

---

####  Save the Best Model

```python
joblib.dump(best_mlp, 'best_mlp.joblib')
```

- Saves the best-trained MLP model to a file using `joblib`, allowing the model to be loaded later for predictions or deployment without retraining.

>  Multi-Layer Perceptrons are highly flexible and can model complex patterns, making them effective for many machine learning tasks, especially in cases where other models (like decision trees or logistic regression) fail to capture the complexity of the data.

### Results Comparison
"""

# Final Model Comparison and Visualization
from matplotlib import pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np

# 1. Prepare Results DataFrame
results_df = pd.DataFrame(results).T
results_df = results_df.sort_values('ROC AUC', ascending=False).round(3)

# 2. Display Formatted Comparison Table
print("""
╔══════════════════════════════════════════════╗
║       OPTIMIZED MODELS PERFORMANCE           ║
╚══════════════════════════════════════════════╝
""")

def highlight_max(s):
    is_max = s == s.max()
    return ['background-color: yellow' if v else '' for v in is_max]

styled_df = results_df.style \
    .apply(highlight_max) \
    .format("{:.3f}") \
    .set_properties(**{'text-align': 'center'}) \
    .set_table_styles([{
        'selector': 'th',
        'props': [('background-color', '#2a3f5f'),
                 ('color', 'white'),
                 ('font-weight', 'bold')]
    }])

display(styled_df)

# 3. Metric Comparison Heatmap
plt.figure(figsize=(14, 8))
sns.heatmap(
    results_df.T,
    annot=True,
    fmt=".3f",
    cmap="YlGnBu",
    linewidths=0.5,
    cbar_kws={'label': 'Score'}
)
plt.title("Optimized Models Performance Comparison", pad=20, fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# 4. ROC Curves Comparison
plt.figure(figsize=(12, 8))
plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.500)')

for model_name, metrics in results.items():
    try:
        model = globals().get(f'best_{model_name.lower().replace(" ", "_")}')
        if model is None:
            continue

        if hasattr(model, 'predict_proba'):
            y_proba = model.predict_proba(X_test)[:, 1]
        elif hasattr(model, 'decision_function'):
            y_proba = model.decision_function(X_test)
        else:
            continue

        fpr, tpr, _ = roc_curve(y_test, y_proba)
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {metrics["ROC AUC"]:.3f})')

    except Exception as e:
        print(f"Skipping {model_name}: {str(e)}")
        continue

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves Comparison')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# 5. Feature Importance (for tree-based models)
tree_models = {
    'Random Forest': best_rf if 'best_rf' in globals() else None,
    'XGBoost': best_xgb if 'best_xgb' in globals() else None,
    'LightGBM': best_lgbm if 'best_lgbm' in globals() else None,
    'Gradient Boosting': best_gb if 'best_gb' in globals() else None
}

valid_models = {k:v for k,v in tree_models.items() if v is not None and hasattr(v, 'feature_importances_')}

if valid_models:
    plt.figure(figsize=(14, 6))
    for i, (name, model) in enumerate(valid_models.items(), 1):
        plt.subplot(1, len(valid_models), i)
        pd.Series(model.feature_importances_, index=X_train.columns) \
          .nlargest(10) \
          .sort_values() \
          .plot(kind='barh')
        plt.title(f'{name} Top Features')
    plt.tight_layout()
    plt.show()
else:
    print("No tree-based models available for feature importance visualization")

"""###  Final Model Comparison and Visualization

In this section, we compare the performance of all optimized models, including a visual representation of their results, using different evaluation metrics like **ROC AUC** and **feature importance**.

---

#### 1. Prepare the Results DataFrame

```python
results_df = pd.DataFrame(results).T
results_df = results_df.sort_values('ROC AUC', ascending=False).round(3)
```

- We create a DataFrame from the `results` dictionary, which contains the evaluation metrics of all models.
- The `.T` transposes the data so that the models are displayed as rows.
- We then sort the models based on the **ROC AUC score** in descending order and round the values to three decimal places for better readability.

---

#### 2. Display Formatted Comparison Table

```python
styled_df = results_df.style \
    .apply(highlight_max) \
    .format("{:.3f}") \
    .set_properties(**{'text-align': 'center'}) \
    .set_table_styles([{
        'selector': 'th',
        'props': [('background-color', '#2a3f5f'),
                 ('color', 'white'),
                 ('font-weight', 'bold')]
    }])
display(styled_df)
```

- We use **styling** techniques to highlight the highest values (e.g., the best-performing models) in the table.
- The `.apply(highlight_max)` function colors the background of the best ROC AUC value in **yellow** for easy identification.
- The table is also formatted with `set_properties` and `set_table_styles` to improve its visual appeal.

---

#### 3. Metric Comparison Heatmap

```python
sns.heatmap(
    results_df.T,
    annot=True,
    fmt=".3f",
    cmap="YlGnBu",
    linewidths=0.5,
    cbar_kws={'label': 'Score'}
)
```

- A **heatmap** is generated using Seaborn, displaying the performance metrics for all models.
- Each cell in the heatmap corresponds to a model's performance across the different evaluation metrics (like ROC AUC, accuracy, etc.).
- The `cmap="YlGnBu"` argument sets the color palette to a gradient from yellow to blue, which helps to distinguish high and low values effectively.

---

#### 4. ROC Curves Comparison

```python
plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.500)')
```

- We plot the **Receiver Operating Characteristic (ROC) curve** for each model.
- The dashed line (`'k--'`) represents a random classifier with an **AUC of 0.500**, which serves as a baseline.
- For each model, we calculate the **False Positive Rate (FPR)** and **True Positive Rate (TPR)**, and plot these curves for comparison.
- The AUC of each model is displayed in the legend to give us an idea of how well each model distinguishes between classes.

---

#### 5. Feature Importance (for Tree-Based Models)

```python
tree_models = {
    'Random Forest': best_rf if 'best_rf' in globals() else None,
    'XGBoost': best_xgb if 'best_xgb' in globals() else None,
    'LightGBM': best_lgbm if 'best_lgbm' in globals() else None,
    'Gradient Boosting': best_gb if 'best_gb' in globals() else None
}
```

- This section focuses on visualizing the **feature importance** for tree-based models, such as **Random Forest**, **XGBoost**, **LightGBM**, and **Gradient Boosting**.
- We first check which models have been successfully trained and contain the `feature_importances_` attribute (common for tree-based models).
- We then plot the **top 10 most important features** for each of these models, which helps us understand which features are driving the model's predictions.

---

#### Output of This Section

This section will give us:
- A **styled comparison table** with models sorted by their ROC AUC performance.
- A **heatmap** that visually compares the models across multiple metrics.
- A **ROC curve comparison** that allows us to visualize the trade-offs between **True Positive Rate** and **False Positive Rate** for each model.
- **Feature importance bar plots** for tree-based models to understand the contribution of each feature to the model's decision-making process.

This final comparison will help us determine which model performs best across various metrics and provide insight into the features that most influence our predictions.

## Key improvements made:

### Comprehensive hyperparameter tuning for all models

### Added feature engineering with interaction terms

### Standardized evaluation metrics

### Model persistence with joblib

### Visual comparison of all models

### Better handling of class imbalance

### More robust preprocessing pipeline

### Parallelized grid search (n_jobs=-1)

### Consistent random states for reproducibility
"""